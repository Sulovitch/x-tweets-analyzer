# =========================================
#               X Tweets Analyzer
#        (One-shot fetch up to 100 only)
# =========================================


# ====== Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª ======
import os, json, re, requests, numpy as np, pandas as pd, matplotlib.pyplot as plt, streamlit as st
from datetime import datetime
from sklearn.linear_model import LinearRegression
from wordcloud import WordCloud
import arabic_reshaper
from bidi.algorithm import get_display

# ====== Ø¶Ø¨Ø· Ù†Ù…Ø· Ø§Ù„Ø±Ø³ÙˆÙ… ======
plt.style.use("dark_background")

# ====== Ø£Ø¯ÙˆØ§Øª ØªÙ†Ø³ÙŠÙ‚ Ø¹Ø±Ø¨ÙŠØ© ======
def reshape_label(text):
    """Ø¥ØµÙ„Ø§Ø­ Ø¹Ø±Ø¶ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù…Ø¹ RTL ÙÙŠ Ø§Ù„Ø±Ø³ÙˆÙ…."""
    try:
        return get_display(arabic_reshaper.reshape(str(text)))
    except Exception:
        return str(text)

def beautify_axes(ax):
    """ØªØ­Ø³ÙŠÙ† Ø´ÙƒÙ„ Ø§Ù„Ø±Ø³ÙˆÙ… (Ø®Ù„ÙÙŠØ© Ø¯Ø§ÙƒÙ†Ø© + Ø£Ù„ÙˆØ§Ù† Ù…Ø­Ø§ÙˆØ±)."""
    ax.set_facecolor("#0E1117")
    ax.tick_params(colors="white")
    ax.xaxis.label.set_color("white")
    ax.yaxis.label.set_color("white")
    ax.title.set_color("#FFD700")

# ====== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ======
st.set_page_config(page_title="ØªØ­Ù„ÙŠÙ„ ØªØºØ±ÙŠØ¯Ø§Øª X", layout="wide")

# ====== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Sidebar) ======


st.sidebar.header("ğŸ”‘ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø¨")

USE_DEMO = st.sidebar.checkbox("ğŸ”„ ØªØ´ØºÙŠÙ„ ÙˆØ¶Ø¹ Ø§Ù„ØªØ¬Ø±Ø¨Ø© ", value=False)

USERNAME = st.sidebar.text_input("ğŸ‘¤ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ø¨Ø¯ÙˆÙ† @)")
BEARER_TOKEN = st.sidebar.text_input("ğŸ”‘ Twitter Bearer Token", type="password")




with st.sidebar.expander("ğŸ“˜ ÙƒÙŠÙ ØªØ­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆÙƒÙ†ØŸ"):
    st.markdown("""
    1) Ø§Ø¯Ø®Ù„ Ø¹Ù„Ù‰ [Twitter Developer Portal](https://developer.twitter.com/en/portal/dashboard)  
    2) Ø£Ù†Ø´Ø¦ App Ø¬Ø¯ÙŠØ¯  
    3) Ù…Ù† **Keys & Tokens** Ø§Ù†Ø³Ø® **Bearer Token**  
    4) Ø£Ù„ØµÙ‚Ù‡ Ù‡Ù†Ø§
    """)

if not USE_DEMO:
    if not USERNAME or not BEARER_TOKEN:
        st.warning("ğŸ‘† Ø£Ø¯Ø®Ù„ Ø§Ø³Ù… Ø§Ù„Ø­Ø³Ø§Ø¨ ÙˆØ§Ù„ØªÙˆÙƒÙ† Ù„Ù„Ù…ØªØ§Ø¨Ø¹Ø©ØŒ Ø£Ùˆ ÙØ¹Ù‘Ù„ ÙˆØ¶Ø¹ Ø§Ù„ØªØ¬Ø±Ø¨Ø©.")
        st.stop()



# ====== Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø­Ù„ÙŠØ© ======
CACHE_FILE = "tweets_cache.json"      # Ø§Ù„ÙƒØ§Ø´ Ù„Ù„ØªØºØ±ÙŠØ¯Ø§Øª + ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ­Ø¯ÙŠØ«
LAST_FETCH_FILE = "last_fetch.json"   # Ø¢Ø®Ø± ÙˆÙ‚Øª Ø¬Ù„Ø¨ (Ù„Ù…Ù†Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø¬Ù„Ø¨ Ù‚Ø¨Ù„ 30 ÙŠÙˆÙ…)

# ====== Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© (Demo Mode) ======
# ====== Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© (Demo Mode) ======
DUMMY_TWEETS = [
    {
        "id": "1",
        "text": "Ø£ÙˆÙ„ ØªØºØ±ÙŠØ¯Ø© ØªØ¬Ø±ÙŠØ¨ÙŠØ© ğŸ˜Š ØªØ¬Ø±Ø¨Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„",
        "created_at": "2025-09-01T12:00:00Z",
        "public_metrics": {
            "like_count": 10,
            "retweet_count": 2,
            "reply_count": 1,
            "impression_count": 100
        },
        "media_urls": []
    },
    {
        "id": "2",
        "text": "ØªØºØ±ÙŠØ¯Ø© Ø«Ø§Ù†ÙŠØ© Ù…Ø¹ ØµÙˆØ±Ø© #ØªØ¬Ø±Ø¨Ø©",
        "created_at": "2025-09-02T18:30:00Z",
        "public_metrics": {
            "like_count": 25,
            "retweet_count": 5,
            "reply_count": 3,
            "impression_count": 200
        },
        "media_urls": ["https://placekitten.com/400/300"]
    },
    {
        "id": "3",
        "text": "Ù†ØµÙŠØ­Ø©: Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© Ù…Ø«Ù„ Ø§Ù„Ø±ÙŠØ§Ø¶Ø©ØŒ Ù„Ø§Ø²Ù… ØªØ¯Ø±ÙŠØ¨ ÙŠÙˆÙ…ÙŠ! ğŸ’» #Ø¨Ø±Ù…Ø¬Ø© #ØªØ¹Ù„Ù…",
        "created_at": "2025-09-03T09:15:00Z",
        "public_metrics": {
            "like_count": 50,
            "retweet_count": 10,
            "reply_count": 5,
            "impression_count": 500
        },
        "media_urls": []
    },
    {
        "id": "4",
        "text": "@example Ø´ÙƒØ±Ø§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ø¹Ù… ğŸ™ ØªØ¬Ø±Ø¨Ø© Ù…Ù†Ø´Ù†",
        "created_at": "2025-09-04T14:45:00Z",
        "public_metrics": {
            "like_count": 5,
            "retweet_count": 0,
            "reply_count": 2,
            "impression_count": 80
        },
        "media_urls": []
    },
    {
        "id": "5",
        "text": "ğŸ”¥ Ø£Ù‡Ù… Ù†ØµØ§Ø¦Ø­ Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ Ø¹Ù„Ù‰ X: Ø§Ù„ØµÙˆØ± + Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨!",
        "created_at": "2025-09-05T21:00:00Z",
        "public_metrics": {
            "like_count": 100,
            "retweet_count": 20,
            "reply_count": 15,
            "impression_count": 1500
        },
        "media_urls": ["https://placebear.com/500/300"]
    },
    {
        "id": "6",
        "text": "Ø§Ù„ÙŠÙˆÙ… ÙƒØ§Ù† Ø¬Ù…ÙŠÙ„ ğŸŒ… #Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© #Ø³Ø¹Ø§Ø¯Ø©",
        "created_at": "2025-09-06T06:30:00Z",
        "public_metrics": {
            "like_count": 80,
            "retweet_count": 8,
            "reply_count": 1,
            "impression_count": 600
        },
        "media_urls": []
    },
    {
        "id": "7",
        "text": "Ù„Ù„Ø£Ø³Ù Ø§Ù„ÙŠÙˆÙ… ÙƒØ§Ù† Ù…Ø²Ø¹Ø¬ Ø¬Ø¯Ù‹Ø§ ğŸ˜ #Ø­Ø²Ù†",
        "created_at": "2025-09-06T23:59:00Z",
        "public_metrics": {
            "like_count": 3,
            "retweet_count": 0,
            "reply_count": 1,
            "impression_count": 120
        },
        "media_urls": []
    },
    {
        "id": "8",
        "text": "ØªØºØ±ÙŠØ¯Ø© ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ø·ÙˆÙŠÙ„Ø© Ø´ÙˆÙŠØ© Ø­ØªÙ‰ Ù†Ø´ÙˆÙ ÙƒÙŠÙ ØªÙ†Ø¹Ø±Ø¶ ÙÙŠ Ø§Ù„Ø¨Ø·Ø§Ù‚Ø©... Ù‡Ø°Ø§ Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø³ÙŠØ· ğŸ‘",
        "created_at": "2025-09-07T11:10:00Z",
        "public_metrics": {
            "like_count": 15,
            "retweet_count": 4,
            "reply_count": 2,
            "impression_count": 300
        },
        "media_urls": []
    },
    {
        "id": "9",
        "text": "Ø¬Ø±Ø¨Øª Ø§Ù„ÙŠÙˆÙ… Ù…ÙƒØªØ¨Ø© Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ù„ØºØ© Ø¨Ø§ÙŠØ«ÙˆÙ† ÙˆÙƒØ§Ù†Øª Ø±Ù‡ÙŠØ¨Ø©! #Python #Coding",
        "created_at": "2025-09-08T17:20:00Z",
        "public_metrics": {
            "like_count": 45,
            "retweet_count": 7,
            "reply_count": 4,
            "impression_count": 450
        },
        "media_urls": []
    },
    {
        "id": "10",
        "text": "Ù…Ø¹Ù„ÙˆÙ…Ø© Ø³Ø±ÙŠØ¹Ø©: ÙŠÙ…ÙƒÙ† ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø³ÙŠØ· Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„ØªÙØ§Ø¹Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Linear Regression ğŸ§ ",
        "created_at": "2025-09-09T13:00:00Z",
        "public_metrics": {
            "like_count": 60,
            "retweet_count": 12,
            "reply_count": 6,
            "impression_count": 900
        },
        "media_urls": []
    }
]



# ====== ØªÙ‡ÙŠØ¦Ø© Ø§ØªØµØ§Ù„ API ======
def auth_header():
    return {"Authorization": f"Bearer {BEARER_TOKEN}"}

def get_user_id(username: str):
    """Ø¬Ù„Ø¨ Ù…Ø¹Ø±Ù‘Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù…Ù† Ø§Ø³Ù…Ù‡."""
    r = requests.get(f"https://api.twitter.com/2/users/by/username/{username}",
                     headers=auth_header(), timeout=30)
    r.raise_for_status()
    return r.json()["data"]["id"]

def get_latest_tweets(user_id: str, max_results: int = 100):
    """
    ÙŠØ¬Ù„Ø¨ Ø­ØªÙ‰ 100 ØªØºØ±ÙŠØ¯Ø© (Ø§Ù„Ø£Ø­Ø¯Ø« Ø£ÙˆÙ„Ø§Ù‹) Ù…Ø¹ Ø±Ø¨Ø· Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù…Ù† includes.media
    ÙˆÙŠØ¯Ø¹Ù… pagination Ø¨Ø´ÙƒÙ„ Ø¢Ù…Ù†. ÙŠØ±Ø¬Ø¹ Ù‚Ø§Ø¦Ù…Ø© Ù…Ø±ØªØ¨Ø© Ù…Ù† Ø§Ù„Ø£Ø­Ø¯Ø« Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù‚Ø¯Ù….
    """
    url = f"https://api.twitter.com/2/users/{user_id}/tweets"
    params = {
        "tweet.fields": "public_metrics,created_at,attachments,referenced_tweets",
        "expansions": "attachments.media_keys",
        "media.fields": "url,preview_image_url,type",
        "max_results": min(max_results, 100)
    }

    all_tweets = {}
    next_token = None

    while len(all_tweets) < max_results:
        if next_token:
            params["pagination_token"] = next_token
        elif "pagination_token" in params:
            params.pop("pagination_token")

        r = requests.get(url, headers=auth_header(), params=params, timeout=30)
        r.raise_for_status()
        data = r.json()

        tweets = data.get("data", [])
        if not tweets:
            break  # âœ… Ù„Ùˆ Ù…Ø§ ÙÙŠÙ‡ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©ØŒ Ù†ÙˆÙ‚Ù

        includes_media = {m["media_key"]: m
                          for m in data.get("includes", {}).get("media", [])} if data.get("includes") else {}

        for t in tweets:
            media_urls = []
            atts = t.get("attachments", {})
            if isinstance(atts, dict) and "media_keys" in atts:
                for mk in atts["media_keys"]:
                    m = includes_media.get(mk)
                    if not m:
                        continue
                    if m.get("type") == "photo" and m.get("url"):
                        media_urls.append(m["url"])
                    elif m.get("type") in ["video", "animated_gif"] and m.get("preview_image_url"):
                        media_urls.append(m["preview_image_url"])
            t["media_urls"] = media_urls
            all_tweets[t["id"]] = t

        next_token = data.get("meta", {}).get("next_token")
        if not next_token:
            break  # âœ… ØªÙˆÙ‚Ù Ø¥Ø°Ø§ Ù…Ø§ ÙÙŠÙ‡ ØµÙØ­Ø© ØªØ§Ù„ÙŠØ©

    # Ø§Ù„Ø£Ø­Ø¯Ø« Ø£ÙˆÙ„Ø§Ù‹
    tweets_list = sorted(all_tweets.values(), key=lambda t: t.get("created_at", ""), reverse=True)
    return tweets_list[:max_results]


def save_cached_tweets(tweets):
    """Ø­ÙØ¸ Ø§Ù„ÙƒØ§Ø´ + Ø®ØªÙ… ÙˆÙ‚Øª Ø§Ù„ØªØ­Ø¯ÙŠØ«."""
    data = {"tweets": tweets, "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def load_cached_tweets():
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØ§Ø´ Ø¥Ù† ÙˆÙØ¬Ø¯."""
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            d = json.load(f)
            return d.get("tweets", []), d.get("last_updated")
    return [], None

# ====== Ø¬Ù„Ø¨ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª (Ù…Ø±Ø© ÙƒÙ„ 30 ÙŠÙˆÙ…) ======
if USE_DEMO:
    # âœ… ÙÙŠ ÙˆØ¶Ø¹ Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø§Ù„ÙˆÙ‡Ù…ÙŠØ©
    tweets = DUMMY_TWEETS
    last_updated = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    last_fetch_date = None
    st.info("ğŸ§ª ÙˆØ¶Ø¹ Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ù…ÙØ¹Ù„ â€” Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù„ÙŠØ³Øª Ø­Ù‚ÙŠÙ‚ÙŠØ©.")
else:
    # ğŸ”„ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø¹Ø§Ø¯ÙŠ: Ù†Ø­Ù…Ù„ Ù…Ù† Ø§Ù„ÙƒØ§Ø´ ÙˆÙ†ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ API
    tweets, last_updated = load_cached_tweets()
    last_fetch_date = None
    if os.path.exists(LAST_FETCH_FILE):
        with open(LAST_FETCH_FILE, "r", encoding="utf-8") as f:
            last_fetch_date = json.load(f).get("last_fetch")

    disable_fetch = False
    if last_fetch_date:
        last_dt = datetime.fromisoformat(last_fetch_date)
        days_since = (datetime.now() - last_dt).days
        if days_since < 30:
            st.info(f"â³ ØªÙ… Ø§Ù„Ø¬Ù„Ø¨ Ø¨ØªØ§Ø±ÙŠØ® {last_dt.strftime('%Y-%m-%d')} â€” ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¬Ù„Ø¨ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¨Ø¹Ø¯ {30 - days_since} ÙŠÙˆÙ…")
            disable_fetch = True
        else:
            st.success("âœ… Ø§Ù†ØªÙ‡Øª Ù…Ø¯Ø© Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø¢Ù†")

    if st.button("ğŸš€ Ø¬Ù„Ø¨ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª (Ø­ØªÙ‰ 100 Ù…Ø±Ù‘Ø© ÙˆØ§Ø­Ø¯Ø©)", disabled=disable_fetch):
        try:
            user_id = get_user_id(USERNAME)
            tweets = get_latest_tweets(user_id, max_results=100)
            save_cached_tweets(tweets)
            with open(LAST_FETCH_FILE, "w", encoding="utf-8") as f:
                json.dump({"last_fetch": datetime.now().isoformat()}, f)
            st.success(f"âœ… ØªÙ… Ø¬Ù„Ø¨ {len(tweets)} ØªØºØ±ÙŠØ¯Ø© Ø¨Ù†Ø¬Ø§Ø­")
        except Exception as e:
            st.error(f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¬Ù„Ø¨: {e}")
            st.stop()


if not tweets:
    st.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯. Ù‚Ù… Ø¨Ø§Ù„Ø¬Ù„Ø¨ Ø£ÙˆÙ„Ø§Ù‹.")
    st.stop()

# ====== ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù€ DataFrame Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ ======
def build_dataframe(raw_tweets):
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø¥Ù„Ù‰ Ø¥Ø·Ø§Ø± Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø£Ø¹Ù…Ø¯Ø© Ù…ÙÙŠØ¯Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„."""
    rows = []
    for t in raw_tweets:
        pm = t.get("public_metrics", {}) or {}
        created_dt = datetime.fromisoformat(t["created_at"].replace("Z", "+00:00")) if t.get("created_at") else None
        created_str = created_dt.strftime("%Y-%m-%d %H:%M") if created_dt else ""

        # ØªØ¹Ø±ÙŠÙ reply: Ø¥Ù…Ø§ ØªØ¨Ø¯Ø£ Ø¨Ù…Ù†Ø´Ù† Ø£Ùˆ ÙÙŠÙ‡Ø§ referenced_tweets Ø¨Ù†ÙˆØ¹ replied_to
        is_reply = False
        if str(t.get("text", "")).strip().startswith("@"):
            is_reply = True
        for ref in t.get("referenced_tweets", []) or []:
            if ref.get("type") == "replied_to":
                is_reply = True
                break

        media_urls = t.get("media_urls", []) or []

        rows.append({
            "id": t.get("id", ""),
            "Ø§Ù„Ù†Øµ": t.get("text", ""),
            "ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±": created_str,
            "DT": created_dt,
            "Ø§Ù„Ø¥Ø¹Ø¬Ø§Ø¨Ø§Øª": pm.get("like_count", 0),
            "Ø§Ù„Ø±ÙŠØªÙˆÙŠØª": pm.get("retweet_count", 0),
            "Ø§Ù„Ø±Ø¯ÙˆØ¯": pm.get("reply_count", 0),
            "Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª": pm.get("impression_count", 0),
            "Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„": pm.get("like_count", 0) + pm.get("retweet_count", 0) + pm.get("reply_count", 0),
            "Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ (%)": 0.0,  # Ù†Ø­ØªØ³Ø¨Ù‡Ø§ Ø¨Ø¹Ø¯ÙŠÙ†
            "has_media": len(media_urls) > 0,
            "media_urls": media_urls,
            "is_reply": is_reply
        })
    df_ = pd.DataFrame(rows)
    # Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙØ§Ø¹Ù„%
    df_["Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ (%)"] = np.where(
        df_["Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª"] > 0,
        (df_["Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"] / df_["Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª"] * 100).round(2),
        0.0
    )
    return df_

df = build_dataframe(tweets)

# ====== ØªØ¨ÙˆÙŠØ¨Ø§Øª Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© ======
tab1, tab2 = st.tabs(["ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙØ§Ø¹Ù„", "ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø³Ø§Ø¨"])

# =========================================================
#                       Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„
# =========================================================
with tab1:
    st.title("ğŸ“Š Ù„ÙˆØ­Ø© ØªØ­ÙƒÙ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª")
    st.caption(f"@{USERNAME} â€” Ø¹Ø¯Ø¯ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª: **{len(df)}** | Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«: **{last_updated or 'â€”'}**")

    # --- ÙÙ„Ø§ØªØ± Ø§Ù„Ø¹Ø±Ø¶ ---
    st.subheader("ğŸ” ÙÙ„Ø§ØªØ±")
    col_f1, col_f2, col_f3 = st.columns(3)
    with col_f1:
        keyword = st.text_input("Ø§Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø© (Ù†Øµ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©)")
    with col_f2:
        date_min = st.date_input("Ù…Ù† ØªØ§Ø±ÙŠØ®", value=(df["DT"].min().date() if df["DT"].notna().any() else datetime.now().date()))
    with col_f3:
        date_max = st.date_input("Ø¥Ù„Ù‰ ØªØ§Ø±ÙŠØ®", value=(df["DT"].max().date() if df["DT"].notna().any() else datetime.now().date()))

    col_f4, col_f5, col_f6 = st.columns(3)
    with col_f4:
        min_eng = st.slider("Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„", 0, int(df["Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"].max() or 0), 0)
    with col_f5:
        kind = st.selectbox("Ù†ÙˆØ¹ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©", ["Ø§Ù„ÙƒÙ„", "ØªØºØ±ÙŠØ¯Ø§Øª Ø£ØµÙ„ÙŠØ© ÙÙ‚Ø·", "Ù…Ù†Ø´Ù† ÙÙ‚Ø·"])
    with col_f6:
        only_media = st.checkbox("ğŸ“· Ø¹Ø±Ø¶ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ ÙˆØ³Ø§Ø¦Ø· ÙÙ‚Ø·", key="filter_media_checkbox")
    


        # âœ… Ø®ÙŠØ§Ø± Ø¥Ø¸Ù‡Ø§Ø±/Ø¥Ø®ÙØ§Ø¡ Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª ÙÙ‚Ø· (Ø§Ù„Ø±Ø³ÙˆÙ… Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¸Ø§Ù‡Ø±Ø©)
    show_cards = st.checkbox(
        "ğŸ—‚ï¸ Ø¹Ø±Ø¶ Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª",
        value=True,
        help="Ø¥Ø°Ø§ Ø£Ù„ØºÙŠØª Ø§Ù„ØªØ­Ø¯ÙŠØ¯ Ø³ÙŠØªÙ… Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ø¨Ø·Ø§Ù‚Ø§Øª ÙÙ‚Ø· â€” Ø§Ù„Ø±Ø³ÙˆÙ… Ø³ØªØ¸Ù„ Ø¸Ø§Ù‡Ø±Ø©."
    )



    # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ±
    filtered = df.copy()
    filtered = filtered[(filtered["DT"].dt.date >= date_min) & (filtered["DT"].dt.date <= date_max)]
    if keyword:
        filtered = filtered[filtered["Ø§Ù„Ù†Øµ"].str.contains(keyword, case=False, na=False)]
    if min_eng > 0:
        filtered = filtered[filtered["Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"] >= min_eng]
    if kind == "ØªØºØ±ÙŠØ¯Ø§Øª Ø£ØµÙ„ÙŠØ© ÙÙ‚Ø·":
        filtered = filtered[filtered["is_reply"] == False]
    elif kind == "Ù…Ù†Ø´Ù† ÙÙ‚Ø·":
        filtered = filtered[filtered["is_reply"] == True]
    if only_media:
        filtered = filtered[filtered["has_media"] == True]

    st.write(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ **{len(filtered)}** ØªØºØ±ÙŠØ¯Ø© Ù…Ø·Ø§Ø¨Ù‚Ø©")

    if filtered.empty:
        st.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ ØªØºØ±ÙŠØ¯Ø§Øª Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¨Ø¹Ø¯ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ±.")
        st.stop()

    # --- Ù…Ø¤Ø´Ø± Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ø¹Ø§Ù… ---
    st.subheader("ğŸ“ˆ Ù…Ø¤Ø´Ø± Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ø¹Ø§Ù… (Engagement Index)")
    st.caption("Ù…Ø¤Ø´Ø± Ø³Ø±ÙŠØ¹: Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„ Ã· Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª Ø¹Ø¨Ø± Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ù…ÙÙ„ØªØ±Ø©.")
    total_eng = int(filtered["Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"].sum())
    total_impr = int(filtered["Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª"].sum())
    ei = (total_eng / total_impr * 100) if total_impr > 0 else 0
    c1, c2, c3 = st.columns(3)
    c1.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„", f"{total_eng:,}")
    c2.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª", f"{total_impr:,}")
    c3.metric("Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„ÙƒÙ„ÙŠØ©", f"{ei:.2f}%")

    # --- Ø¹Ø±Ø¶ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª ÙƒØ¨Ø·Ø§Ù‚Ø§Øª ---
    st.subheader("ğŸ“ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª")
    st.caption("ÙƒÙ„ Ø¨Ø·Ø§Ù‚Ø© ØªØ¹Ø±Ø¶ Ø£Ù‡Ù… Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ØªØºØ±ÙŠØ¯Ø© Ù…Ø¹ Ø±Ø§Ø¨Ø· Ù…Ø¨Ø§Ø´Ø± ÙˆØµÙˆØ± Ø¥Ù† ÙˆØ¬Ø¯Øª.")

    # --- Ø¹Ø±Ø¶ Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª ---
    if show_cards:
        for _, row in filtered.iterrows():
            tweet_url = f"https://twitter.com/{USERNAME}/status/{row['id']}"
            st.markdown(
                f"""
                <div style='direction: rtl; text-align: right; border:1px solid #444; border-radius:10px; padding:10px; margin-bottom:8px;'>
                <b>Ø§Ù„Ù†Øµ:</b> {row['Ø§Ù„Ù†Øµ']}<br>
                <b>ğŸ“… ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±:</b> {row['ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±']}<br>
                â¤ï¸ <b>{row['Ø§Ù„Ø¥Ø¹Ø¬Ø§Ø¨Ø§Øª']}</b> | ğŸ” <b>{row['Ø§Ù„Ø±ÙŠØªÙˆÙŠØª']}</b> | ğŸ’¬ <b>{row['Ø§Ù„Ø±Ø¯ÙˆØ¯']}</b> |
                ğŸ‘€ <b>{row['Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª']}</b> | ğŸ“Š <b>Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {row['Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„']}</b> | % <b>{row['Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ (%)']}</b><br>
                ğŸ”— <a href="{tweet_url}" target="_blank">ÙØªØ­ Ø§Ù„ØªØºØ±ÙŠØ¯Ø© Ø¹Ù„Ù‰ X</a>
                </div>
                """,
                unsafe_allow_html=True
            )
            if row["has_media"]:
                for m in row["media_urls"]:
                    st.image(m, use_container_width=True)
    else:
        st.info("ğŸ—‚ï¸ ØªÙ… Ø¥Ø®ÙØ§Ø¡ Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª. ÙØ¹Ù‘Ù„ Ø§Ù„Ø®ÙŠØ§Ø± Ø£Ø¹Ù„Ø§Ù‡ Ù„Ø¹Ø±Ø¶Ù‡Ø§.")

    # âœ… Ø§Ù„Ø±Ø³ÙˆÙ… Ø£ØµØ¨Ø­Øª Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¸Ø§Ù‡Ø±Ø© Ø¨ØºØ¶ Ø§Ù„Ù†Ø¸Ø± Ø¹Ù† show_cards

    st.subheader("ğŸ”¥ Ø£ÙƒØ«Ø± Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª ØªÙØ§Ø¹Ù„Ù‹Ø§")
    st.caption("Ø£ÙØ¶Ù„ 10 ØªØºØ±ÙŠØ¯Ø§Øª Ø­Ø³Ø¨ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„.")
    top10 = filtered.sort_values(by="Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„", ascending=False).head(10)
    if len(top10) > 0:
        fig, ax = plt.subplots()
        ax.barh([reshape_label(str(t)[:50]) for t in top10["Ø§Ù„Ù†Øµ"]], top10["Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"])
        ax.set_xlabel(reshape_label("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„")); ax.set_ylabel(reshape_label("Ø§Ù„Ù†Øµ"))
        beautify_axes(ax)
        st.pyplot(fig)
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ ØªØºØ±ÙŠØ¯Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„Ø±Ø³Ù….")

    st.subheader("â° Ø£ÙØ¶Ù„ Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ù†Ø´Ø± (Ù…ØªÙˆØ³Ø· Ø§Ù„ØªÙØ§Ø¹Ù„)")
    st.caption("Ù…ØªÙˆØ³Ø· Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„ Ù„ÙƒÙ„ Ø³Ø§Ø¹Ø© Ù†Ø´Ø±.")
    if filtered["DT"].notna().any():
        filtered["Ø³Ø§Ø¹Ø©"] = filtered["DT"].dt.hour
        hourly = filtered.groupby("Ø³Ø§Ø¹Ø©")["Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"].mean()
        fig2, ax2 = plt.subplots()
        hourly.plot(kind="bar", ax=ax2)
        ax2.set_xlabel(reshape_label("Ø³Ø§Ø¹Ø© Ø§Ù„Ù†Ø´Ø±")); ax2.set_ylabel(reshape_label("Ù…ØªÙˆØ³Ø· Ø§Ù„ØªÙØ§Ø¹Ù„"))
        beautify_axes(ax2)
        st.pyplot(fig2)
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙˆØ§Ø±ÙŠØ® ØµØ§Ù„Ø­Ø©.")

    st.subheader("ğŸ“ˆ Ø£Ø¹Ù„Ù‰ Ù†Ø³Ø¨ Ø§Ù„ØªÙØ§Ø¹Ù„")
    st.caption("Ø£ÙØ¶Ù„ 10 ØªØºØ±ÙŠØ¯Ø§Øª Ø­Ø³Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ (Ø§Ù„ØªÙØ§Ø¹Ù„ Ã· Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª).")
    top_rate = filtered.sort_values(by="Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ (%)", ascending=False).head(10)
    if len(top_rate) > 0:
        fig3, ax3 = plt.subplots()
        ax3.barh([reshape_label(str(t)[:50]) for t in top_rate["Ø§Ù„Ù†Øµ"]], top_rate["Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ (%)"])
        ax3.set_xlabel(reshape_label("Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ (%)"))
        beautify_axes(ax3)
        st.pyplot(fig3)
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ©.")

    # --- Heatmap ---
    st.subheader("ğŸ“… Ø£ÙØ¶Ù„ Ø§Ù„Ø£ÙˆÙ‚Ø§Øª Ù„Ù„Ù†Ø´Ø± (Heatmap)")
    st.caption("Ù…ØªÙˆØ³Ø· Ø§Ù„ØªÙØ§Ø¹Ù„ Ø­Ø³Ø¨ Ø§Ù„ÙŠÙˆÙ… ÙˆØ§Ù„Ø³Ø§Ø¹Ø©.")
    tmp = filtered.copy()
    if not tmp.empty:
        if "ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±_DT" not in tmp.columns:
            tmp["ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±_DT"] = pd.to_datetime(tmp["ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±"], errors="coerce")
        tmp["Ø§Ù„ÙŠÙˆÙ…_Ø§Ù†Ø¬Ù„ÙŠØ²ÙŠ"] = tmp["ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±_DT"].dt.day_name()
        tmp["Ø§Ù„ÙŠÙˆÙ…"] = tmp["Ø§Ù„ÙŠÙˆÙ…_Ø§Ù†Ø¬Ù„ÙŠØ²ÙŠ"].map({
            "Sunday": "Ø§Ù„Ø£Ø­Ø¯", "Monday": "Ø§Ù„Ø§Ø«Ù†ÙŠÙ†", "Tuesday": "Ø§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡",
            "Wednesday": "Ø§Ù„Ø£Ø±Ø¨Ø¹Ø§Ø¡", "Thursday": "Ø§Ù„Ø®Ù…ÙŠØ³", "Friday": "Ø§Ù„Ø¬Ù…Ø¹Ø©", "Saturday": "Ø§Ù„Ø³Ø¨Øª"
        })
        tmp["Ø§Ù„Ø³Ø§Ø¹Ø©"] = tmp["ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±_DT"].dt.hour
        pivot_table = tmp.pivot_table(index="Ø§Ù„ÙŠÙˆÙ…", columns="Ø§Ù„Ø³Ø§Ø¹Ø©", values="Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„", aggfunc="mean", fill_value=0)
        if pivot_table.empty:
            st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ø¥Ù†Ø´Ø§Ø¡ Heatmap.")
        else:
            days_order = ["Ø§Ù„Ø£Ø­Ø¯","Ø§Ù„Ø§Ø«Ù†ÙŠÙ†","Ø§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡","Ø§Ù„Ø£Ø±Ø¨Ø¹Ø§Ø¡","Ø§Ù„Ø®Ù…ÙŠØ³","Ø§Ù„Ø¬Ù…Ø¹Ø©","Ø§Ù„Ø³Ø¨Øª"]
            pivot_table = pivot_table.reindex(days_order)
            fig_hm, ax_hm = plt.subplots(figsize=(10, 5))
            cax = ax_hm.imshow(pivot_table, cmap="YlOrRd", aspect="auto")
            ax_hm.set_yticks(range(len(pivot_table.index)))
            ax_hm.set_yticklabels([reshape_label(day) for day in pivot_table.index])
            ax_hm.set_xticks(range(len(pivot_table.columns)))
            ax_hm.set_xticklabels([reshape_label(str(col)) for col in pivot_table.columns], rotation=90)
            ax_hm.set_xlabel(reshape_label("Ø§Ù„Ø³Ø§Ø¹Ø©")); ax_hm.set_ylabel(reshape_label("Ø§Ù„ÙŠÙˆÙ…"))
            ax_hm.set_title(reshape_label("Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªÙØ§Ø¹Ù„ Ø­Ø³Ø¨ Ø§Ù„ÙŠÙˆÙ… ÙˆØ§Ù„Ø³Ø§Ø¹Ø©"))
            fig_hm.colorbar(cax, ax=ax_hm, label=reshape_label("Ù…ØªÙˆØ³Ø· Ø§Ù„ØªÙØ§Ø¹Ù„"))
            beautify_axes(ax_hm)
            st.pyplot(fig_hm)
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ©.")

    # --- ØªÙˆÙ‚Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ ---
    st.subheader("ğŸ”® ØªÙˆÙ‚Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ")
    st.caption("Ø¹Ù„Ø§Ù‚Ø© Ø®Ø·ÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª ÙˆØ§Ù„ØªÙØ§Ø¹Ù„ Ù„ØªÙ‚Ø¯ÙŠØ± Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹.")
    if len(filtered) >= 4 and filtered["Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª"].sum() > 0:
        X = np.array(filtered["Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª"]).reshape(-1, 1)
        y = np.array(filtered["Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"])
        try:
            model = LinearRegression().fit(X, y)
            st.info(f"Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø±: {model.coef_[0]:.4f} | Ø§Ù„Ø«Ø§Ø¨Øª: {model.intercept_:.2f}")
            future_impr = st.number_input("Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©:", min_value=0, value=500, step=50)
            pred = model.predict([[future_impr]])[0]
            st.success(f"Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {pred:.0f}")

            fig_lr, ax_lr = plt.subplots()
            ax_lr.scatter(X, y, label=reshape_label("Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª"))
            x_line = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
            ax_lr.plot(x_line, model.predict(x_line), label=reshape_label("Ø®Ø· Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø±"))
            ax_lr.set_xlabel(reshape_label("Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª")); ax_lr.set_ylabel(reshape_label("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"))
            ax_lr.legend()
            beautify_axes(ax_lr)
            st.pyplot(fig_lr)
        except Exception as e:
            st.warning(f"ØªØ¹Ø°Ù‘Ø± ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø±: {e}")
    else:
        st.info("ØªØ­ØªØ§Ø¬ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ 4 ØªØºØ±ÙŠØ¯Ø§Øª Ø°Ø§Øª Ù…Ø´Ø§Ù‡Ø¯Ø§Øª > 0 Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.")

    # --- Ø³Ø­Ø§Ø¨Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª ---
    st.subheader("â˜ï¸ Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‹Ø§")
    st.caption("ÙŠØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆØ§Ù„Ù…Ù†Ø´Ù† ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙˆÙ‚ÙÙŠØ© Ù‚Ø¨Ù„ Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±.")
    word_counts = {}
    if not filtered.empty:
        for _, row in filtered.iterrows():
            text = re.sub(r"http\S+|www\S+|@\S+", "", str(row["Ø§Ù„Ù†Øµ"]))
            words = text.split()
            for w in words:
                w = w.strip().lower()
                if w.startswith("Ø§Ù„"):
                    w = w[2:]
                if w in {"ÙÙŠ","Ø¹Ù„Ù‰","Ù…Ù†","Ø¹Ù†","Ø§Ù„Ù‰","Ø¥Ù„Ù‰","Ùˆ","Ø§Ùˆ","Ø£Ùˆ","Ù…Ø§","Ù„Ø§","Ù‡Ø°Ø§","Ù‡Ø°Ù‡","Ø°Ù„Ùƒ","Ù‡Ø°ÙŠ","Ù‡Ø°ÙŠÙƒ"}:
                    continue
                if w:
                    word_counts[reshape_label(w)] = word_counts.get(reshape_label(w), 0) + 1

    if word_counts:
        try:
            wordcloud = WordCloud(font_path="arial.ttf", width=1200, height=600,
                                background_color="white", max_words=100, min_font_size=14,
                                colormap="plasma").generate_from_frequencies(word_counts)
            fig_wc, ax_wc = plt.subplots(figsize=(12, 6))
            ax_wc.imshow(wordcloud, interpolation="bilinear"); ax_wc.axis("off")
            st.pyplot(fig_wc)
        except Exception:
            st.info("ØªØ¹Ø°Ù‘Ø± ØªÙˆÙ„ÙŠØ¯ Ø³Ø­Ø§Ø¨Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª (ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø®Ø· Ø¹Ø±Ø¨ÙŠ Ù…Ø«Ù„ arial.ttf).")

        st.markdown("### ğŸ† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹")
        top_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:20]
        top_df = pd.DataFrame(top_words, columns=["Ø§Ù„ÙƒÙ„Ù…Ø©", "Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª"])
        top_df["Ø§Ù„ÙƒÙ„Ù…Ø©"] = top_df["Ø§Ù„ÙƒÙ„Ù…Ø©"].apply(reshape_label)
        st.markdown(top_df.to_html(index=False, justify="right"), unsafe_allow_html=True)
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ ÙƒÙ„Ù…Ø§Øª ÙƒØ§ÙÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ.")

   

# =========================================================
#                       ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø³Ø§Ø¨
# =========================================================
with tab2:
    st.title("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø´Ø§Ù…Ù„")
    st.info("Ù‡Ø°Ù‡ Ø§Ù„ØªØ¨ÙˆÙŠØ¨Ø© ØªØ¹Ø±Ø¶ ØªØ­Ù„ÙŠÙ„Ø§Øª Ù…Ø¹Ù…Ù‘Ù‚Ø©: Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„ÙƒØªØ§Ø¨Ø©ØŒ Ø§Ù„Ù‡Ø§Ø´ØªØ§Ù‚Ø§ØªØŒ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·ØŒ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±ØŒ ÙˆØ§Ù„Ù€ n-grams.")

    # --- ØªØ¬Ù‡ÙŠØ² DF Ù…Ø®ØµØµ Ù„Ù„ØªØ­Ù„ÙŠÙ„ (Ù…Ø¨Ù†ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙÙ„Ø§ØªØ± Ù…Ù† Ø§Ù„ØªØ§Ø¨ 1) ---
    @st.cache_data
    def prepare_profile_dataframe(tweets_local):
        rows = []
        for t in tweets_local:
            created_dt = datetime.fromisoformat(t["created_at"].replace("Z", "+00:00")) if t.get("created_at") else None
            created_str = created_dt.strftime("%Y-%m-%d %H:%M") if created_dt else ""
            txt = str(t.get("text", ""))

            rows.append({
                "id": t.get("id", ""),
                "Ø§Ù„Ù†Øµ": txt,
                "ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±": created_str,
                "DT": created_dt,
                "hashtags": re.findall(r"#(\w+)", txt),
                "mentions": re.findall(r"@(\w+)", txt),
                "contains_link": bool(re.search(r"http\S+", txt)),
                "links": re.findall(r"http[s]?://[^\s]+", txt),
                "is_reply": str(txt).strip().startswith("@"),
                "has_media": len(t.get("media_urls", []) or []) > 0,
                "media_urls": t.get("media_urls", []) or []
            })
        return pd.DataFrame(rows)

    # âœ… Ø§Ø³ØªØ®Ø¯Ù… ÙÙ‚Ø· Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø§Ù„Ù…ÙÙ„ØªØ±Ø©
    filtered_ids = set(filtered["id"].astype(str))
    tweets_filtered_raw = [t for t in tweets if str(t.get("id", "")) in filtered_ids]
    pdf = prepare_profile_dataframe(tweets_filtered_raw)

    # ğŸ”’ Ø£Ø¶Ù ÙØ­Øµ Ù‡Ù†Ø§ Ù‚Ø¨Ù„ Ø£ÙŠ Ø±Ø³Ù… Ø£Ùˆ ØªØ­Ù„ÙŠÙ„
    if pdf.empty:
        st.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¨Ø¹Ø¯ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ± Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª.")
        st.stop()

    
    # =============== ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± =============== 
        
    # --- Ù‚Ø§Ù…ÙˆØ³ Ù…Ø´Ø§Ø¹Ø± Ø¹Ø±Ø¨ÙŠ Ù…ÙˆØ³Ù‘Ø¹ Ù…Ø¹ ÙƒÙ„Ù…Ø§Øª Ø¹Ø§Ù…ÙŠØ© ---
    AR_POS = {
        # Ø±Ø³Ù…ÙŠ + ÙØµÙŠØ­
        "Ù…Ù…ØªØ§Ø²","Ø¬Ù…ÙŠÙ„","Ø±Ø§Ø¦Ø¹","Ø£ÙØ¶Ù„","Ù…Ù…ÙŠØ²","Ø´ÙƒØ±Ø§","Ø³Ø¹ÙŠØ¯","Ù…Ø­Ø¨ÙˆØ¨","Ù†Ø¬Ø§Ø­","Ø­Ù„Ùˆ","ÙØ§Ø®Ø±","Ù‚ÙˆÙŠ","Ù…ÙÙŠØ¯",
        "Ù…Ø¨Ù‡Ø¬","Ù…ÙØ±Ø­","Ù…Ø³Ø±ÙˆØ±","Ù…Ø±ÙŠØ­","Ù„Ø·ÙŠÙ","Ù…Ø·Ù…Ø¦Ù†","Ù…Ø¨Ù‡Ø±","Ù…Ø°Ù‡Ù„","Ø¹Ø¸ÙŠÙ…","Ø³ÙˆØ¨Ø±","Ø¹Ø¨Ù‚Ø±ÙŠ","Ù…Ø«Ø§Ù„ÙŠ",
        "Ù…Ø­ØªØ±Ù…","Ø±Ø§Ù‚ÙŠ","Ø¨Ø·Ù„","Ù…ÙƒØ³Ø¨","Ù…ØªÙÙˆÙ‚","Ù‡ÙŠØ¨Ø©","ÙØ±Ø­Ø©","Ù…Ø±Ø­","Ù…Ø¨Ø±ÙˆÙƒ","Ù…ÙˆÙÙ‚","Ø±Ø§Ø¨Ø­","Ù…Ø±Ø¨Ø­",
        "Ø³Ø§Ø­Ø±","Ù…Ø´Ø±Ù‚","Ù…Ù…ÙŠØ²","Ù‚ÙˆÙŠ","Ø´Ø¬Ø§Ø¹","Ø¥ÙŠØ¬Ø§Ø¨ÙŠ","Ù…Ø±ÙŠØ­","Ø¬Ù…ÙŠÙ„","Ù…ØªØ­Ù…Ø³","Ù…Ø³Ø±Ø­ÙŠ","Ø³Ù„Ø§Ù…","Ø­Ø¨","Ø®ÙŠØ±",
        "ÙˆØ§Ùˆ","Ù…Ø°Ù‡ÙˆÙˆÙ„","ÙÙ„","Ø®Ø±Ø§ÙÙŠ","Ø§Ø³Ø·ÙˆØ±ÙŠ","Ø®ÙˆØ±Ø§ÙÙŠ","Ø¬Ù†Ø§Ù†","Ø±ÙˆØ¹Ø©","ØªØ­ÙØ©","Ø®ÙŠØ§Ù„","Ù‚Ù†Ø¨Ù„Ø©","ÙØ§ÙŠÙ_Ø³ØªØ§Ø±",
        "Ø¬Ø§Ù…Ø¯","ÙƒÙÙˆ","Ø³Ø·ÙˆØ±ÙŠ","ÙÙ†Ø§Ù†","Ø¨Ø·Ù„","Ø­Ù…Ø§Ø³","ÙÙ„Ø©","Ù…Ø²ÙŠØ§Ù†","Ø¬ÙŠØ¯","Ø±Ø§ÙŠÙ‚","Ø¹Ø§Ù„Ù…ÙŠ","ØªÙˆØ¨","Ø³ÙˆØ¨Ø±","Ù‚ÙˆÙŠÙŠÙŠ",
        "ÙÙ„Ù‘Ø©","Ù‚Ù„Ø¨","ÙƒÙˆÙŠØ³","Ø²ÙŠÙ†","Ø·Ø±Ø¨","Ø·Ù‚Ø·Ù‚Ù‡ Ø­Ù„ÙˆÙ‡","ÙÙ†Ø§Ù†","Ù…Ù…ØªØ§Ø²Ø©","Ù…Ø±ØªØ¨","Ø±Ù‡ÙŠØ¨","Ø´ÙŠÙƒ","Ù…Ù‡ÙŠØ¨","Ø¹Ø¬Ø¨Ù†ÙŠ"
    }

    AR_NEG = {
        # Ø±Ø³Ù…ÙŠ + ÙØµÙŠØ­
        "Ø³ÙŠØ¡","Ø³Ø¦","Ø±Ø¯ÙŠØ¡","Ø£Ø³ÙˆØ£","Ø­Ø²ÙŠÙ†","Ø®Ø³Ø§Ø±Ø©","ÙØ´Ù„","Ù…Ø²ÙŠÙ","Ø¶Ø¹ÙŠÙ","Ø²Ø¹Ø¬","Ø¥Ø²Ø¹Ø§Ø¬","ÙƒØ§Ø±Ø«ÙŠ","ØºÙ„Ø·",
        "Ù…Ø²Ø¹Ø¬","Ù…ØªØ¹Ø¨","Ù…Ù‚Ø±Ù","Ù…Ù…Ù„","Ù…Ø¤Ù„Ù…","Ù…Ø­Ø¨Ø·","Ø¨Ø´Ø¹","Ø­Ù‚Ø¯","ÙƒØ±Ø§Ù‡ÙŠØ©","Ø¹Ø¯Ø§Ø¡","Ù…Ø£Ø³Ø§Ø©","ÙƒØ§Ø±Ø«Ø©","Ø³Ù„Ø¨ÙŠØ©",
        "Ø¶ÙŠØ§Ø¹","Ù‚Ù„Ø©","ÙÙˆØ¶Ù‰","Ø¬Ø±Ø­","Ø®ÙˆÙ","Ù…ØµÙŠØ¨Ø©","ÙƒØ³Ø±","Ø¹Ù†Ù","Ø¸Ù„Ù…","Ù…Ø´ÙƒÙ„Ø©","Ø¶Ø¹Ù","Ù‡Ø²ÙŠÙ…Ø©","ÙƒØ¦ÙŠØ¨",
        "Ù…Ø¹Ø§Ù†Ø§Ø©","Ø¨Ø§Ø¦Ø³","Ù…Ù‚Ø²Ø²","Ø³Ù…","Ø£Ø°Ù‰","Ù…Ø±Ø¹Ø¨","ØºØ¶Ø¨","Ù„Ø¹Ù†Ø©","Ø¹Ø§Ø±","Ø®ÙŠØ¨Ø©","ÙØ¶ÙŠØ­Ø©","Ù…Ø£Ø³Ø§ÙˆÙŠ","Ù…Ø²Ø¹Ø¬Ø©",
        # Ø¹Ø§Ù…ÙŠØ© + ØªÙˆÙŠØªØ±
        "Ù‚Ù‡Ø±","Ø·ÙØ´","Ø®Ø§ÙŠØ³","Ø¨Ø§ÙŠØ®","Ø²ÙØª","Ø®Ø§ÙŠØµ","Ø´ÙŠÙ†","ØªØ¹Ø¨","Ù‡Ù…","Ù†ÙƒØ¯","Ù…Ù‚Ù„Ø¨","ØºØ¨Ø§Ø¡","Ø®Ø±Ø§Ø¨","ØªØ§ÙÙ‡",
        "Ù…Ø³Ø®Ø±Ø©","Ø¨Ù„Ø§Ø¡","Ù†Ø±ÙØ²Ø©","Ù‚Ø±Ù","ØªØ¹Ø¨Ø§Ù†","Ø·ÙØ´Ø§Ù†","ÙŠØ§ Ø³Ø§ØªØ±","Ø¨Ø±Ø¨Ø³Ø©","Ù…ØºØ¨ÙˆÙ†","Ù‚Ø­Ø·","Ù…Ù„Ù„","Ù…Ù„Ù„","Ø¯Ù…Ø§Ø±",
        "Ø·ÙŠØ­Ù†ÙŠ","ÙØ´Ù„Ù†ÙŠ","Ù…Ø·ÙÙˆÙ‚","Ù‚Ø±ÙØ§Ù†","Ø³Ù…Ø¬","ÙƒØ±ÙŠÙ‡","Ù†Ø±ÙØ²","Ø®Ø±Ø§","Ø²Ø¹Ù„Ø§Ù†","Ù…Ù†Ø­ÙˆØ³","Ù…Ø®ÙŠØ³","Ø®Ø§Ø²ÙˆÙ‚","Ø´Ù†ÙŠØ¹"
    }

    EN_POS = {
    "good","great","awesome","amazing","love","like","nice","happy","perfect","excellent","cool",
    "win","strong","helpful","fantastic","wonderful","brilliant","super","genius","epic","respect",
    "best","top","success","profit","joy","fun","wow","outstanding","positive","cheer","bless","safe",
    "bright","hero","masterpiece","congrats","peace","hope","gift","advantage","benefit","enjoy","smile",
    }

    EN_NEG = {
        "bad","worse","worst","sad","angry","hate","disappoint","fake","weak","annoy","terrible","awful",
        "fail","loss","mess","broken","pain","boring","useless","stupid","lazy","ugly","trouble","risk",
        "fear","danger","negative","cry","problem","miss","chaos","harm","toxic","slow","sucks","hate","mad",
        "angry","depress","scary","nightmare","down","damage","fraud","spam","junk","block","poor",
    }

    def normalize_ar(text: str) -> str:
        s = str(text)

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆØ§Ù„Ù…Ù†Ø´Ù† ÙˆØ§Ù„Ù‡Ø§Ø´ØªØ§Ù‚Ø§Øª
        s = re.sub(r"http\S+|www\S+|@\S+", " ", s)
        s = re.sub(r"#", " ", s)

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ ÙˆØ§Ù„Ù…Ø¯ÙˆØ¯
        s = re.sub(r"[\u0617-\u061A\u064B-\u0652\u0670\u06D6-\u06ED\u0640]", "", s)

        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù‡Ù…Ø²Ø§Øª
        s = s.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")

        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„ÙŠØ§Ø¡ ÙˆØ§Ù„Ø£Ù„Ù Ø§Ù„Ù…Ù‚ØµÙˆØ±Ø©
        s = s.replace("Ù‰", "ÙŠ")

        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù‡Ø§Ø¡ ÙˆØ§Ù„ØªØ§Ø¡ Ø§Ù„Ù…Ø±Ø¨ÙˆØ·Ø©
        s = s.replace("Ø©", "Ù‡")

        # Ø¥Ø²Ø§Ù„Ø© ØªÙƒØ±Ø§Ø± Ø§Ù„Ø­Ø±ÙˆÙ Ø£ÙƒØ«Ø± Ù…Ù† Ù…Ø±ØªÙŠÙ† (Ø±Ø§Ø§Ø§Ø§Ø§Ø¦Ø¹ â†’ Ø±Ø§Ø¦Ø¹)
        s = re.sub(r"(.)\1{2,}", r"\1", s)

        # Ù…Ø³Ø§ÙØ§Øª Ù†Ø¸ÙŠÙØ©
        s = re.sub(r"\s+", " ", s).strip().lower()
        return s


    def simple_sentiment(text: str) -> int:
        """ÙŠØ±Ø¬Ø¹: 1 Ø¥ÙŠØ¬Ø§Ø¨ÙŠØŒ -1 Ø³Ù„Ø¨ÙŠØŒ 0 Ù…Ø­Ø§ÙŠØ¯."""
        t = normalize_ar(text)
        toks = re.findall(r"[a-zA-Z\u0600-\u06FF]+", t)
        score = 0
        for w in toks:
            lw = w.lower()
            if lw in AR_POS or lw in EN_POS: score += 1
            if lw in AR_NEG or lw in EN_NEG: score -= 1
        if score > 0: return 1
        if score < 0: return -1
        return 0


    # =============== ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ø²Ù…Ù†ÙŠ ===============
    st.subheader("ğŸ•’ Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ø²Ù…Ù†ÙŠ")
    st.caption("ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø¹Ù„Ù‰ Ø£ÙŠØ§Ù… Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ ÙˆØ³Ø§Ø¹Ø§Øª Ø§Ù„ÙŠÙˆÙ… + Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØºØ±ÙŠØ¯ Ø§Ù„ÙŠÙˆÙ…ÙŠ.")
    if not pdf.empty:
        pdf["Ø§Ù„ÙŠÙˆÙ…"] = pdf["DT"].dt.day_name()
        day_map = {"Sunday":"Ø§Ù„Ø£Ø­Ø¯", "Monday":"Ø§Ù„Ø§Ø«Ù†ÙŠÙ†", "Tuesday":"Ø§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡",
                   "Wednesday":"Ø§Ù„Ø£Ø±Ø¨Ø¹Ø§Ø¡", "Thursday":"Ø§Ù„Ø®Ù…ÙŠØ³", "Friday":"Ø§Ù„Ø¬Ù…Ø¹Ø©", "Saturday":"Ø§Ù„Ø³Ø¨Øª"}
        pdf["Ø§Ù„ÙŠÙˆÙ…"] = pdf["Ø§Ù„ÙŠÙˆÙ…"].map(day_map)

        fig_day, ax_day = plt.subplots()
        pdf["Ø§Ù„ÙŠÙˆÙ…"].value_counts().reindex(day_map.values()).plot(kind="bar", ax=ax_day)
        ax_day.set_xticklabels([reshape_label(lbl.get_text()) for lbl in ax_day.get_xticklabels()])
        ax_day.set_xlabel(reshape_label("Ø§Ù„ÙŠÙˆÙ…")); ax_day.set_ylabel(reshape_label("Ø¹Ø¯Ø¯ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª"))
        ax_day.set_title(reshape_label("ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø¹Ù„Ù‰ Ø£ÙŠØ§Ù… Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹"))
        beautify_axes(ax_day); st.pyplot(fig_day)

        pdf["Ø³Ø§Ø¹Ø©"] = pdf["DT"].dt.hour
        fig_hour, ax_hour = plt.subplots()
        pdf["Ø³Ø§Ø¹Ø©"].value_counts().sort_index().plot(kind="bar", ax=ax_hour)
        ax_hour.set_xticklabels([reshape_label(lbl.get_text()) for lbl in ax_hour.get_xticklabels()])
        ax_hour.set_xlabel(reshape_label("Ø§Ù„Ø³Ø§Ø¹Ø©")); ax_hour.set_ylabel(reshape_label("Ø¹Ø¯Ø¯ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª"))
        ax_hour.set_title(reshape_label("ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø¹Ù„Ù‰ Ø³Ø§Ø¹Ø§Øª Ø§Ù„ÙŠÙˆÙ…"))
        beautify_axes(ax_hour); st.pyplot(fig_hour)

        tweets_per_day = pdf.groupby(pdf["DT"].dt.date).size()
        st.metric("Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØºØ±ÙŠØ¯ ÙŠÙˆÙ…ÙŠÙ‹Ø§", f"{tweets_per_day.mean():.2f} ØªØºØ±ÙŠØ¯Ø©/ÙŠÙˆÙ…")
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ±.")

    # =============== ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ ===============
    # =============== ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ (Ù…Ø·ÙˆØ±) ===============
    st.subheader("ğŸ“ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨")
    st.caption("ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ‘Ù„ Ù„Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„ÙƒØªØ§Ø¨Ø©: Ø·ÙˆÙ„ Ø§Ù„Ù†ØµÙˆØµØŒ Ø§Ù„ÙƒÙ„Ù…Ø§ØªØŒ Ø§Ù„Ø¬Ù…Ù„ØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ù…ÙˆØ²ØŒ Ø§Ù„Ù‡Ø§Ø´ØªØ§Ù‚Ø§ØªØŒ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·ØŒ ØªÙ†ÙˆØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§ØªØŒ ÙˆØ§Ù„Ù…Ø´Ø§Ø¹Ø±.")

    style_df = filtered.copy()  # âœ… Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙÙ„ØªØ±Ø©
    if not style_df.empty:
        # --- Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø£Ø³Ø§Ø³ÙŠØ© ---
        style_df["Ø·ÙˆÙ„_Ø§Ù„Ù†Øµ"] = style_df["Ø§Ù„Ù†Øµ"].astype(str).str.len()
        style_df["Ø¹Ø¯Ø¯_Ø§Ù„ÙƒÙ„Ù…Ø§Øª"] = style_df["Ø§Ù„Ù†Øµ"].astype(str).str.split().apply(len)
        style_df["Ø¹Ø¯Ø¯_Ø§Ù„Ø¬Ù…Ù„"] = style_df["Ø§Ù„Ù†Øµ"].str.count(r"[.!ØŸ!]")
        style_df["ÙÙŠÙ‡_Ø³Ø¤Ø§Ù„"] = style_df["Ø§Ù„Ù†Øµ"].str.contains(r"\?", regex=True)
        style_df["ÙÙŠÙ‡_ØªØ¹Ø¬Ø¨"] = style_df["Ø§Ù„Ù†Øµ"].str.contains(r"!", regex=True)
        style_df["ÙÙŠÙ‡_Ù†Ù‚Ø·ØªÙŠÙ†"] = style_df["Ø§Ù„Ù†Øµ"].str.contains(":")
        style_df["Ø¹Ø¯Ø¯_Ø§Ù„Ù‡Ø§Ø´ØªØ§Ù‚Ø§Øª"] = style_df["Ø§Ù„Ù†Øµ"].str.count(r"#\w+")
        style_df["Ø¹Ø¯Ø¯_Ø§Ù„Ø±ÙˆØ§Ø¨Ø·"] = style_df["Ø§Ù„Ù†Øµ"].str.count(r"http[s]?://")
        emoji_pattern = r"[\U0001F300-\U0001FAD6\U0001F900-\U0001F9FF\U00002600-\U000026FF]"
        style_df["Ø¹Ø¯Ø¯_Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ"] = style_df["Ø§Ù„Ù†Øµ"].str.count(emoji_pattern)

        # --- Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ---
        c1, c2, c3 = st.columns(3)
        c1.metric("Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©", f"{style_df['Ø·ÙˆÙ„_Ø§Ù„Ù†Øµ'].mean():.1f} Ø­Ø±Ù")
        c2.metric("Ù…ØªÙˆØ³Ø· Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª", f"{style_df['Ø¹Ø¯Ø¯_Ø§Ù„ÙƒÙ„Ù…Ø§Øª'].mean():.1f}")
        c3.metric("Ù†Ø³Ø¨Ø© Ø§Ù„Ø±Ø¯ÙˆØ¯", f"{style_df['is_reply'].mean()*100:.1f}%")

        c4, c5, c6 = st.columns(3)
        c4.metric("Ù…ØªÙˆØ³Ø· Ø¹Ø¯Ø¯ Ø§Ù„Ø¬Ù…Ù„", f"{style_df['Ø¹Ø¯Ø¯_Ø§Ù„Ø¬Ù…Ù„'].mean():.1f} Ø¬Ù…Ù„Ø©")
        c5.metric("Ù…ØªÙˆØ³Ø· Ø¹Ø¯Ø¯ Ø§Ù„Ù‡Ø§Ø´ØªØ§Ù‚Ø§Øª", f"{style_df['Ø¹Ø¯Ø¯_Ø§Ù„Ù‡Ø§Ø´ØªØ§Ù‚Ø§Øª'].mean():.2f}")
        c6.metric("Ù…ØªÙˆØ³Ø· Ø¹Ø¯Ø¯ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·", f"{style_df['Ø¹Ø¯Ø¯_Ø§Ù„Ø±ÙˆØ§Ø¨Ø·'].mean():.2f}")

        c7, c8, c9 = st.columns(3)
        c7.metric("Ù†Ø³Ø¨Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¹Ø¬Ø¨", f"{style_df['ÙÙŠÙ‡_ØªØ¹Ø¬Ø¨'].mean()*100:.1f}%")
        c8.metric("Ù†Ø³Ø¨Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø¤Ø§Ù„", f"{style_df['ÙÙŠÙ‡_Ø³Ø¤Ø§Ù„'].mean()*100:.1f}%")
        c9.metric("Ù†Ø³Ø¨Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù‚Ø·ØªÙŠÙ†", f"{style_df['ÙÙŠÙ‡_Ù†Ù‚Ø·ØªÙŠÙ†'].mean()*100:.1f}%")

        # --- Ù…Ø¤Ø´Ø± ØªÙ†ÙˆØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ---
        all_words = " ".join(style_df["Ø§Ù„Ù†Øµ"].astype(str)).split()
        unique_words = set(all_words)
        lexical_diversity = len(unique_words) / len(all_words) if len(all_words) > 0 else 0
        st.metric("ğŸ“Š Ù…Ø¤Ø´Ø± ØªÙ†ÙˆØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª", f"{lexical_diversity*100:.1f}%")

        # --- Ø±Ø³Ù… ØªÙˆØ²ÙŠØ¹ Ø·ÙˆÙ„ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª ---
        st.caption("ØªÙˆØ²ÙŠØ¹ Ø·ÙˆÙ„ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª (Ø­Ø±ÙˆÙ).")
        fig_len, ax_len = plt.subplots()
        style_df["Ø·ÙˆÙ„_Ø§Ù„Ù†Øµ"].plot(kind="hist", bins=20, ax=ax_len)
        ax_len.set_xlabel(reshape_label("Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ")); ax_len.set_ylabel(reshape_label("Ø¹Ø¯Ø¯ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª"))
        beautify_axes(ax_len); st.pyplot(fig_len)

        # --- ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹ ---
        import re
        all_emojis = "".join(re.findall(emoji_pattern, " ".join(style_df["Ø§Ù„Ù†Øµ"].astype(str))))
        from collections import Counter
        emoji_counts = Counter(all_emojis)
        if emoji_counts:
            st.markdown("### ğŸ˜ Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹")
            top_emoji = pd.DataFrame(emoji_counts.most_common(10), columns=["Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ","Ø§Ù„ØªÙƒØ±Ø§Ø±"])
            st.dataframe(top_emoji)
        else:
            st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¥ÙŠÙ…ÙˆØ¬ÙŠØ§Øª ÙƒØ§ÙÙŠØ© Ù„Ø¹Ø±Ø¶Ù‡Ø§.")

        # --- Ù…Ù„Ø®Øµ Ù…Ø´Ø§Ø¹Ø± Ø³Ø±ÙŠØ¹ Ø¯Ø§Ø®Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ ---
        st.markdown("### ğŸ˜Š ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± (Ù…Ø¨Ø³Ù‘Ø·)")
        style_df["sentiment"] = style_df["Ø§Ù„Ù†Øµ"].apply(simple_sentiment)
        sent_dist = style_df["sentiment"].map({1:"Ø¥ÙŠØ¬Ø§Ø¨ÙŠ",0:"Ù…Ø­Ø§ÙŠØ¯",-1:"Ø³Ù„Ø¨ÙŠ"}).value_counts(normalize=True)
        st.bar_chart(sent_dist)
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ Ø¨Ø¹Ø¯ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ±.")
    

    # --- Ø¹Ø±Ø¶ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© ÙˆØ§Ù„Ø³Ù„Ø¨ÙŠØ© Ø§Ù„ÙØ¹Ù„ÙŠØ© Ù…Ù† Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª ---
    from collections import Counter

    pos_counter = Counter()
    neg_counter = Counter()

    # --- Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø± ÙÙ‚Ø· ---
    for txt in style_df["Ø§Ù„Ù†Øµ"]:
        normalized = normalize_ar(txt)
        words = re.findall(r"[a-zA-Z\u0600-\u06FF]+", normalized)
        for w in words:
            if w in AR_POS:
                pos_counter[w] += 1
            elif w in AR_NEG:
                neg_counter[w] += 1

    # --- Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· ---
    st.subheader("ğŸ“Š Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© ÙˆØ§Ù„Ø³Ù„Ø¨ÙŠØ© ÙÙŠ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø§Ù„Ù…ÙÙ„ØªØ±Ø©")
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("### âœ… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹")
        if pos_counter:
            pos_df = pd.DataFrame(pos_counter.most_common(20), columns=["Ø§Ù„ÙƒÙ„Ù…Ø©","Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±"])
            st.dataframe(pos_df)
        else:
            st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ ÙƒÙ„Ù…Ø§Øª Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© ÙÙŠ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø§Ù„Ù…ÙÙ„ØªØ±Ø©.")

    with col2:
        st.markdown("### âŒ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø³Ù„Ø¨ÙŠØ© Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹")
        if neg_counter:
            neg_df = pd.DataFrame(neg_counter.most_common(20), columns=["Ø§Ù„ÙƒÙ„Ù…Ø©","Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±"])
            st.dataframe(neg_df)
        else:
            st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ ÙƒÙ„Ù…Ø§Øª Ø³Ù„Ø¨ÙŠØ© ÙÙŠ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø§Ù„Ù…ÙÙ„ØªØ±Ø©.")




    # =============== ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· ===============
    st.subheader("ğŸ¥ ØªØ£Ø«ÙŠØ± Ø§Ù„ÙˆØ³Ø§Ø¦Ø·")

    media_grp = filtered.groupby("has_media")["Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"].mean().rename({False: "Ø¨Ø¯ÙˆÙ† ÙˆØ³Ø§Ø¦Ø·", True: "Ù…Ø¹ ÙˆØ³Ø§Ø¦Ø·"})

    if not media_grp.empty:
        fig_media, ax_media = plt.subplots()
        media_grp.plot(kind="bar", ax=ax_media)
        ax_media.set_xlabel(reshape_label("Ø§Ù„ÙˆØ³Ø§Ø¦Ø·"))
        ax_media.set_ylabel(reshape_label("Ù…ØªÙˆØ³Ø· Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"))
        beautify_axes(ax_media)
        st.pyplot(fig_media)
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø¨Ø¹Ø¯ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ±.")


    # =============== ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‡Ø§Ø´ØªØ§Ù‚Ø§Øª ===============
    st.subheader("ğŸ·ï¸ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù‡Ø§Ø´ØªØ§Ù‚Ø§Øª")
    tag_rows = []
    for _, r in filtered.iterrows():  # âœ… Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ÙÙ„ØªØ±
        tags = re.findall(r"#(\w+)", str(r["Ø§Ù„Ù†Øµ"]))
        for t in tags:
            tag_rows.append({"hashtag": t.lower(), "eng": r["Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"]})
    if tag_rows:
        tag_df = pd.DataFrame(tag_rows)
        top_use = tag_df["hashtag"].value_counts().head(15)
        st.bar_chart(top_use)

        perf = tag_df.groupby("hashtag")["eng"].mean().sort_values(ascending=False).head(15)
        st.markdown("### ğŸ” Ø£ÙØ¶Ù„ 15 Ù‡Ø§Ø´ØªØ§Ù‚ Ø­Ø³Ø¨ **Ù…ØªÙˆØ³Ø· Ø§Ù„ØªÙØ§Ø¹Ù„**")
        st.dataframe(perf.reset_index().rename(columns={"hashtag":"Ø§Ù„Ù‡Ø§Ø´ØªØ§Ù‚","eng":"Ù…ØªÙˆØ³Ø· Ø§Ù„ØªÙØ§Ø¹Ù„"}))
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù‡Ø§Ø´ØªØ§Ù‚Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„.")



    # =============== ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ===============
    st.subheader("ğŸ”— ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·")
    st.caption("Ø£ÙƒØ«Ø± Ø§Ù„Ù†Ø·Ø§Ù‚Ø§Øª ØªÙƒØ±Ø§Ø±Ù‹Ø§ ÙÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ ØªØºØ±ÙŠØ¯Ø§ØªÙƒ.")
    all_links = [l for links in pdf["links"] for l in links]
    if all_links:
        domains = pd.Series([re.sub(r"https?://(www\.)?", "", l).split("/")[0] for l in all_links])
        dom_counts = domains.value_counts().head(15)
        st.bar_chart(dom_counts)
        st.markdown("### Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù†Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ù‹Ø§")
        st.dataframe(dom_counts.reset_index().rename(columns={"index":"Ø§Ù„Ù†Ø·Ø§Ù‚", 0:"Ø¹Ø¯Ø¯ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·"}))
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø±ÙˆØ§Ø¨Ø· ÙƒØ§ÙÙŠØ©.")

    # =============== Bigram / Trigram ===============
    st.subheader("ğŸ“š Ø£ÙƒØ«Ø± Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª ØªÙƒØ±Ø§Ø±Ø§Ù‹ (Bigram / Trigram)")
    st.caption("Ù†Ø·Ø¨Ù‘Ø¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ ÙˆÙ†Ø²ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆØ§Ù„Ù…Ù†Ø´Ù† ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙˆÙ‚ÙÙŠØ©ØŒ Ø«Ù… Ù†Ø¹Ø±Ø¶ Ø£ÙƒØ«Ø± Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø´ÙŠÙˆØ¹Ø§Ù‹.")

    from collections import Counter

    AR_STOP = {
        "ÙÙŠ","Ø¹Ù„Ù‰","Ù…Ù†","Ø¹Ù†","Ø§Ù„Ù‰","Ø¥Ù„Ù‰","Ùˆ","Ø§Ùˆ","Ø£Ùˆ","Ù…Ø§","Ù„Ø§","Ù‡Ø°Ø§","Ù‡Ø°Ù‡","Ø°Ù„Ùƒ","Ù‡Ø°ÙŠ","Ù‡Ø°ÙŠÙƒ",
        "Ø§Ù†Ø§","Ø£Ù†Ø§","Ø§Ù†Øª","Ø£Ù†Øª","Ù‡Ùˆ","Ù‡ÙŠ","Ù‡Ù…","Ù‡Ù†","Ù…Ø¹","ØªÙ…","Ø¹Ù†","Ù‚Ø¯","ÙƒØ§Ù†","ÙƒØ§Ù†Øª","ÙƒÙŠÙ","Ù„ÙŠØ´","Ù„ÙŠÙ‡",
        "the","a","an","and","or","to","of","for","in","on","with","is","are","am","it","this","that"
    }

    def normalize_ar(text: str) -> str:
        s = str(text)
        s = re.sub(r"http\S+|www\S+|@\S+", " ", s)
        s = re.sub(r"#", " ", s)
        s = re.sub(r"[\u0617-\u061A\u064B-\u0652\u0670\u06D6-\u06ED\u0640]", "", s)
        s = s.replace("Ø£","Ø§").replace("Ø¥","Ø§").replace("Ø¢","Ø§").replace("Ù‰","ÙŠ").replace("Ø¤","Ùˆ").replace("Ø¦","ÙŠ").replace("Ø©","Ù‡")
        s = re.sub(r"\s+", " ", s).strip().lower()
        return s

    def tokenize(text: str):
        s = normalize_ar(text)
        tokens = re.findall(r"[a-zA-Z\u0600-\u06FF]+", s)
        return [t for t in tokens if len(t) >= 2 and t not in AR_STOP]

    def ngrams_from_tokens(tokens, n=2):
        return [" ".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)] if len(tokens) >= n else []

    def count_ngrams(series_text, n=2) -> Counter:
        c = Counter()
        for txt in series_text:
            toks = tokenize(txt)
            c.update(ngrams_from_tokens(toks, n=n))
        return c

    @st.cache_data
    def count_ngrams_cached(series_text, n=2):
        return count_ngrams(series_text, n)

    def plot_top_counter(counter: Counter, title: str, k: int = 10):
        items = counter.most_common(k)
        if not items:
            st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ø¨Ø§Ø±Ø§Øª ÙƒØ§ÙÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ.")
            return
        labels = [reshape_label(t) for t, _ in items]
        values = [v for _, v in items]
        fig, ax = plt.subplots()
        ax.barh(labels, values)
        ax.set_xlabel(reshape_label("Ø§Ù„ØªÙƒØ±Ø§Ø±"))
        ax.set_title(reshape_label(title))
        beautify_axes(ax)
        st.pyplot(fig)
        st.dataframe(pd.DataFrame(items, columns=["Ø§Ù„Ø¹Ø¨Ø§Ø±Ø©","Ø§Ù„ØªÙƒØ±Ø§Ø±"]))

    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯ÙˆØ§Ù„
    bigrams = count_ngrams_cached(pdf["Ø§Ù„Ù†Øµ"], n=2)
    plot_top_counter(bigrams, "Ø£ÙƒØ«Ø± Bigram ØªÙƒØ±Ø§Ø±Ø§Ù‹", k=10)

    trigrams = count_ngrams_cached(pdf["Ø§Ù„Ù†Øµ"], n=3)
    plot_top_counter(trigrams, "Ø£ÙƒØ«Ø± Trigram ØªÙƒØ±Ø§Ø±Ø§Ù‹", k=10)






    # =============== Ø´Ø¨ÙƒØ© Ø§Ù„Ù…Ù†Ø´Ù† (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) ===============
    st.subheader("ğŸ§© Ø´Ø¨ÙƒØ© Ø§Ù„Ù…Ù†Ø´Ù†")
    st.caption("ØªØ¹Ø±Ø¶ Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… Ø°ÙƒØ±Ù‡Ø§ Ø¨ÙƒØ«Ø±Ø©. (ØªØ­ØªØ§Ø¬ Ù…ÙƒØªØ¨Ø© pyvisØ› Ø³ÙŠØªÙ… Ø¹Ø±Ø¶ ØªÙ†Ø¨ÙŠÙ‡ Ø¥Ù† Ù„Ù… ØªØªÙˆÙØ±).")
    try:
        from pyvis.network import Network
        import streamlit.components.v1 as components
        import matplotlib.colors as mcolors
        import matplotlib.cm as cm

        mention_rows = []
        for _, row in pdf.iterrows():
            for m in row["mentions"]:
                mention_rows.append({"source": USERNAME, "target": m})

        if not mention_rows:
            st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù†Ø´Ù†Ø§Øª ÙƒØ§ÙÙŠØ©.")
        else:
            mdf = pd.DataFrame(mention_rows)
            mention_counts = mdf["target"].value_counts()
            top_n = st.slider("ğŸ” Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶Ø©", 5, 50, 15)
            selected_mentions = mention_counts.head(top_n)
            st.write(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ **{len(selected_mentions)}** Ø­Ø³Ø§Ø¨ Ù„Ù„Ø¹Ø±Ø¶")
            st.bar_chart(selected_mentions)

            net = Network(height="750px", width="100%", bgcolor="#0E1117", font_color="white", directed=True)
            net.add_node(USERNAME, label=f"@{USERNAME}", size=50, color="#FFD700", shape="dot")

            max_count = selected_mentions.max() if len(selected_mentions) > 0 else 1
            norm = plt.Normalize(vmin=selected_mentions.min(), vmax=max_count)
            cmap = cm.get_cmap("coolwarm")

            for account, count in selected_mentions.items():
                net.add_node(
                    account,
                    label=account,
                    size=20 + count * 3,
                    color=mcolors.to_hex(cmap(norm(count))),
                    shape="dot",
                    title=f"Ù…Ø±Ø§Øª Ø§Ù„Ø°ÙƒØ±: {count}"
                )
                net.add_edge(USERNAME, account, color="#AAAAAA", width=2)

            net.save_graph("mentions_network.html")
            st.success("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø³Ù… â€” ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ¹Ø±Ø§Ø¶Ù‡ Ø£Ø¯Ù†Ø§Ù‡ Ø£Ùˆ ØªØ­Ù…ÙŠÙ„Ù‡.")
            with open("mentions_network.html", "r", encoding="utf-8") as f:
                components.html(f.read(), height=800)
            with open("mentions_network.html", "rb") as f:
                st.download_button("ğŸ’¾ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø±Ø³Ù… ÙƒÙ€ HTML", data=f, file_name="mentions_network.html", mime="text/html")
    except Exception:
        st.warning("âš ï¸ Ù„ØªÙØ¹ÙŠÙ„ Ø´Ø¨ÙƒØ© Ø§Ù„Ù…Ù†Ø´Ù†ØŒ Ø«Ø¨Ù‘Øª:  `pip install pyvis`")


