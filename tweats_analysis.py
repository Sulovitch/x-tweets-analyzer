# =========================================
#        X Tweets Analyzer (One Page)
#   OAuth2 PKCE + Full Analytics Dashboard
# =========================================

# =========================================
#        X Tweets Analyzer (One Page)
#   OAuth2 PKCE + Full Analytics Dashboard
# =========================================

import os, json, requests, secrets, urllib.parse, base64, hashlib
import numpy as np, pandas as pd, matplotlib.pyplot as plt, streamlit as st
from datetime import datetime, timedelta
from sklearn.linear_model import LinearRegression
from wordcloud import WordCloud
import arabic_reshaper
from bidi.algorithm import get_display
from collections import Counter
import sqlite3
import re
import matplotlib.pyplot as plt
from matplotlib import rcParams
import json
import time




plt.rcParams['font.family'] = ['Arial', 'Segoe UI Emoji']




def init_db():
    conn = sqlite3.connect("tweets.db")
    c = conn.cursor()

    # Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª
    c.execute("""
        CREATE TABLE IF NOT EXISTS tweets (
            id TEXT PRIMARY KEY,
            user_id TEXT,
            username TEXT,
            text TEXT,
            created_at TEXT,
            like_count INTEGER,
            retweet_count INTEGER,
            reply_count INTEGER,
            impression_count INTEGER,
            media_urls TEXT
        )
    """)

    # ØªØ£ÙƒØ¯ Ù…Ù† media_urls Ù„Ùˆ Ø¬Ø¯ÙˆÙ„ Ù‚Ø¯ÙŠÙ…
    c.execute("PRAGMA table_info(tweets)")
    cols = [col[1] for col in c.fetchall()]
    if "media_urls" not in cols:
        try:
            c.execute("ALTER TABLE tweets ADD COLUMN media_urls TEXT")
        except Exception:
            pass

    # Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªÙˆÙƒÙ†Ø§Øª (Ù„Ø§Ø²Ù… ÙŠÙƒÙˆÙ† Ø¨Ø±Ø§ Ø§Ù„Ù€ if)
    c.execute("""
        CREATE TABLE IF NOT EXISTS user_tokens (
            user_id TEXT PRIMARY KEY,
            username TEXT,
            access_token TEXT,
            refresh_token TEXT,
            expires_at TEXT,
            last_fetch TEXT
        )
    """)

    # ØªØ£ÙƒØ¯ Ù…Ù† last_fetch Ù„Ùˆ Ø¬Ø¯ÙˆÙ„ Ù‚Ø¯ÙŠÙ…
    c.execute("PRAGMA table_info(user_tokens)")
    cols = [col[1] for col in c.fetchall()]
    if "last_fetch" not in cols:
        try:
            c.execute("ALTER TABLE user_tokens ADD COLUMN last_fetch TEXT")
        except Exception:
            pass

    # Ø¬Ø¯ÙˆÙ„ oauth_state (Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ø¨Ø³ Ù…ÙÙŠØ¯)
    c.execute("""
        CREATE TABLE IF NOT EXISTS oauth_state (
            state TEXT PRIMARY KEY,
            verifier TEXT
        )
    """)

    conn.commit()
    conn.close()
init_db()




def save_tokens_to_db(user_id, username, tokens):
    """Ø§Ø­ÙØ¸ access/refresh + ÙˆÙ‚Øª Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ ÙÙŠ SQLite"""
    expires_at = None
    try:
        # Ø¨Ø¹Ø¶ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª ØªØ±Ø¬Ø¹ expires_in Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ
        if "expires_in" in tokens:
            expires_at = (datetime.now() + timedelta(seconds=int(tokens["expires_in"]))).isoformat()
    except Exception:
        pass

    conn = sqlite3.connect("tweets.db")
    c = conn.cursor()
    c.execute("""
    INSERT OR REPLACE INTO user_tokens (user_id, username, access_token, refresh_token, expires_at)
    VALUES (?, ?, ?, ?, ?)
    """, (
        user_id,
        username,
        tokens.get("access_token"),
        tokens.get("refresh_token"),
        expires_at
    ))
    conn.commit()
    conn.close()

def save_state_to_db(state, verifier):
    conn = sqlite3.connect("tweets.db")
    c = conn.cursor()
    c.execute("""
    CREATE TABLE IF NOT EXISTS oauth_state (
        state TEXT PRIMARY KEY,
        verifier TEXT
    )
    """)
    c.execute("INSERT OR REPLACE INTO oauth_state (state, verifier) VALUES (?, ?)", (state, verifier))
    conn.commit()
    conn.close()

def load_state_from_db(state):
    conn = sqlite3.connect("tweets.db")
    c = conn.cursor()
    c.execute("SELECT verifier FROM oauth_state WHERE state=?", (state,))
    row = c.fetchone()
    conn.close()
    return row[0] if row else None


def load_tokens_from_db_by_user(user_id):
    conn = sqlite3.connect("tweets.db")
    c = conn.cursor()
    c.execute("SELECT access_token, refresh_token, expires_at, username FROM user_tokens WHERE user_id=?", (user_id,))
    row = c.fetchone()
    conn.close()
    if not row:
        return None
    return {
        "access_token": row[0],
        "refresh_token": row[1],
        "expires_at": row[2],
        "username": row[3]
    }


def load_any_tokens():
    """Ù„Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø´Ø®ØµÙŠ (Ù…Ø³ØªØ®Ø¯Ù… ÙˆØ§Ø­Ø¯): Ø±Ø¬Ù‘Ø¹ Ø£ÙˆÙ„ ØµÙ Ù…Ø­ÙÙˆØ¸ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ø¨Ø¹Ø¯ Refresh Ø§Ù„ØµÙØ­Ø©."""
    conn = sqlite3.connect("tweets.db")
    c = conn.cursor()
    c.execute("SELECT user_id, username, access_token, refresh_token, expires_at FROM user_tokens LIMIT 1")
    row = c.fetchone()
    conn.close()
    if not row:
        return None
    return {
        "user_id": row[0],
        "username": row[1],
        "access_token": row[2],
        "refresh_token": row[3],
        "expires_at": row[4]
    }


def refresh_access_token(refresh_token):
    token_url = "https://api.twitter.com/2/oauth2/token"
    data = {
        "grant_type": "refresh_token",
        "refresh_token": refresh_token,
        "client_id": CLIENT_ID,
    }
    headers = {"Content-Type": "application/x-www-form-urlencoded"}
    r = requests.post(token_url, data=data, headers=headers, timeout=30)
    return r


def ensure_valid_token():
    """
    - Ø¥Ø°Ø§ Ù…Ø§ ÙÙŠÙ‡ access_token Ø¨Ø§Ù„Ø¬Ù„Ø³Ø© â†’ Ø®Ù„Ù‡ ÙØ§Ø¶ÙŠ ÙˆØ®Ù„ÙŠ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ³Ø¬Ù„ Ø¯Ø®ÙˆÙ„.
    - Ø¥Ø°Ø§ ÙÙŠÙ‡ expires_at + refresh_token â†’ ÙŠØ­Ø¯Ø«Ù‡ Ø¹Ø§Ø¯ÙŠ Ø²ÙŠ Ø£ÙˆÙ„.
    """
    # Ù„Ø§ ØªØ­Ù…Ù„ Ø£ÙŠ Ø­Ø³Ø§Ø¨ Ù…Ø­ÙÙˆØ¸ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
    if "access_token" not in st.session_state:
        return  

    # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ© (ØªÙ‚Ø±ÙŠØ¨ÙŠ) Ø«Ù… Ø­Ø¯Ù‘Ø«
    exp = st.session_state.get("expires_at")
    ref = st.session_state.get("refresh_token")
    if exp and ref:
        try:
            if datetime.fromisoformat(exp) <= datetime.now():
                rr = refresh_access_token(ref)
                if rr.status_code == 200:
                    new_tokens = rr.json()
                    st.session_state["access_token"] = new_tokens["access_token"]

                    # Ø­Ø¯Ø« expires_at
                    if "expires_in" in new_tokens:
                        st.session_state["expires_at"] = (
                            datetime.now() + timedelta(seconds=int(new_tokens["expires_in"]))
                        ).isoformat()

                    # Ø®Ø²Ù‘Ù† Ø¨Ø§Ù„Ø¯Ø§ØªØ§Ø¨ÙŠØ³ (Ø¥Ø°Ø§ ÙÙŠÙ‡ user_id/username)
                    if st.session_state.get("user_id") and st.session_state.get("username"):
                        save_tokens_to_db(
                            st.session_state["user_id"],
                            st.session_state["username"],
                            {
                                "access_token": new_tokens["access_token"],
                                "refresh_token": new_tokens.get("refresh_token", ref),
                                "expires_in": new_tokens.get("expires_in"),
                            },
                        )
                else:
                    st.warning("Ø§Ù†ØªÙ‡Øª ØµÙ„Ø§Ø­ÙŠØ© Ø§Ù„ØªÙˆÙƒÙ† ÙˆØªØ¹Ø°Ù‘Ø± ØªØ­Ø¯ÙŠØ«Ù‡. Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ù…Ù† Ø¬Ø¯ÙŠØ¯.")
                    for k in ["access_token", "refresh_token", "expires_at", "user_id", "username"]:
                        st.session_state.pop(k, None)
        except Exception:
            pass


def get_last_fetch(user_id):
    conn = sqlite3.connect("tweets.db")
    c = conn.cursor()
    c.execute("SELECT last_fetch FROM user_tokens WHERE user_id=?", (user_id,))
    row = c.fetchone()
    conn.close()
    return row[0] if row and row[0] else None

def format_last_fetch(last_fetch_date):
    if last_fetch_date and last_fetch_date not in ["â€” (ÙˆØ¶Ø¹ ØªØ¬Ø±ÙŠØ¨ÙŠ)"]:
        try:
            dt = datetime.fromisoformat(last_fetch_date)
            return dt.strftime("%Y-%m-%d %H:%M")
        except Exception:
            return last_fetch_date
    return last_fetch_date or "â€”"



# ====== Ø¶Ø¨Ø· Ù†Ù…Ø· Ø§Ù„Ø±Ø³ÙˆÙ… ======
plt.style.use("dark_background")

# ====== Ø£Ø¯ÙˆØ§Øª ØªÙ†Ø³ÙŠÙ‚ Ø¹Ø±Ø¨ÙŠØ© ======
def reshape_label(text):
    try:
        return get_display(arabic_reshaper.reshape(str(text)))
    except Exception:
        return str(text)

def beautify_axes(ax):
    ax.set_facecolor("#0E1117")
    ax.tick_params(colors="white")
    ax.xaxis.label.set_color("white")
    ax.yaxis.label.set_color("white")
    ax.title.set_color("#FFD700")

# ====== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ØªÙˆÙŠØªØ± OAuth ======
CLIENT_ID = st.secrets["CLIENT_ID"]
REDIRECT_URI = st.secrets["REDIRECT_URI"]
SCOPE = "tweet.read users.read offline.access"
AUTH_URL = "https://twitter.com/i/oauth2/authorize"

# ====== Ù…Ø®Ø²Ù† Ù„Ø­ÙØ¸ state/verifier ======
@st.cache_resource
def get_state_store():
    return {}

state_store = get_state_store()

# ====== ØªÙˆÙ„ÙŠØ¯ PKCE ======
def gen_pkce_pair():
    code_verifier = secrets.token_urlsafe(64)
    digest = hashlib.sha256(code_verifier.encode()).digest()
    code_challenge = base64.urlsafe_b64encode(digest).decode().rstrip("=")
    return code_verifier, code_challenge

# ====== Ø³ØªØ§ÙŠÙ„ Ø²Ø± Ø§Ù„Ø¯Ø®ÙˆÙ„ ======
st.markdown(
    """
    <style>
    div.stButton > button {
        background-color: #1DA1F2;
        color: white;
        font-size: 22px;
        font-weight: bold;
        padding: 15px 40px;
        border-radius: 10px;
        border: none;
        cursor: pointer;
        transition: 0.3s;
    }
    div.stButton > button:hover { 
        background-color: #0d8ddb; 
    }
    .center-button {
        display: flex;
        justify-content: center;   /* ÙˆØ³Ø· Ø£ÙÙ‚ÙŠØ§Ù‹ */
        align-items: center;       /* ÙˆØ³Ø· Ø¹Ù…ÙˆØ¯ÙŠØ§Ù‹ */
                    
    }
    </style>
    """,
    unsafe_allow_html=True
)

# ====== Ø´Ø§Ø´Ø© ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ + ÙˆØ¶Ø¹ Ø§Ù„ØªØ¬Ø±Ø¨Ø© (Ù…Ø¹ ÙˆØ³Ø§Ø¦Ø· ÙˆÙ‡Ù…ÙŠØ©) ======
if "access_token" not in st.session_state:
    with st.container():
        st.markdown('<div class="center-button">', unsafe_allow_html=True)

        # Ø²Ø± ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø§Ù„Ø¹Ø§Ø¯ÙŠ
        if st.button("ğŸ”‘ Ø³Ø¬Ù„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¨Ø­Ø³Ø§Ø¨ ØªÙˆÙŠØªØ±"):
            state = secrets.token_urlsafe(16)
            code_verifier, code_challenge = gen_pkce_pair()
            save_state_to_db(state, code_verifier)

            params = {
                "response_type": "code",
                "client_id": CLIENT_ID,
                "redirect_uri": REDIRECT_URI,
                "scope": SCOPE,
                "state": state,
                "code_challenge": code_challenge,
                "code_challenge_method": "S256",
                "prompt": "consent",
            }
            login_url = f"{AUTH_URL}?{urllib.parse.urlencode(params)}"
            st.markdown(
                f"""<meta http-equiv="refresh" content="0; url={login_url}">""",
                unsafe_allow_html=True,
            )

       # Ø²Ø± ÙˆØ¶Ø¹ Ø§Ù„ØªØ¬Ø±Ø¨Ø© (demo mode) ØªØ­Øª Ø²Ø± ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„
    if st.button("ğŸ‘€ ÙˆØ¶Ø¹ Ø§Ù„ØªØ¬Ø±Ø¨Ø© (Ø¹Ø±Ø¶ ØªØ¬Ø±ÙŠØ¨ÙŠ)"):
        import random
        

        now = datetime.now()

        # Ø¬Ù…Ù„ Ù…ØªÙ†ÙˆØ¹Ø© Ù„Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø§Ù„ÙˆÙ‡Ù…ÙŠØ©
        samples = [
            "ØªØ­Ø¯ÙŠØ« Ø¨Ø³ÙŠØ· Ø¹Ù† Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: Ø§Ù„Ø£Ù…ÙˆØ± ØªØªÙ‚Ø¯Ù… Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ© ğŸ› ï¸",
            "Ø§ÙˆÙ„Ø§Ù‹: Ø´ÙƒØ±Ù‹Ø§ Ù„Ù„ÙØ±ÙŠÙ‚ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ù‡Ø¯ Ø§Ù„ÙŠÙˆÙ… ğŸ™",
            "Ù…Ø¹Ù„ÙˆÙ…Ø© Ø³Ø±ÙŠØ¹Ø©: Ø§Ù„ØªÙ‚Ù†ÙŠØ© ØªØºÙŠØ± Ø´ÙƒÙ„ Ø§Ù„Ø¹Ù…Ù„ Ø¨Ø³Ø±Ø¹Ø© âš¡",
            "ÙÙƒØ±Ø©: Ù…Ø§Ø°Ø§ Ù„Ùˆ Ø±Ø¨Ø·Ù†Ø§ Ø§Ù„Ø®Ø¯Ù…Ø© Ù…Ø¹ API Ø¢Ø®Ø±ØŸ ğŸ¤”",
            "Ù„Ø­Ø¸Ø© Ø±ÙŠØ§Ø¶ÙŠØ©: Ù…Ø¨Ø±ÙˆÙƒ Ù„Ù„ÙØ±ÙŠÙ‚ Ø¹Ù„Ù‰ Ø§Ù„ÙÙˆØ² ğŸ†",
            "Ù†ØµÙŠØ­Ø©: Ø§Ø¨Ø¯Ø£ ÙŠÙˆÙ…Ùƒ Ø¨Ø®Ø·Ø© ØµØºÙŠØ±Ø© ÙˆØ³Ø±ÙŠØ¹Ø© Ø§Ù„ØªÙ†ÙÙŠØ° âœ…",
            "Ù„Ù‚Ø·Ø© Ù…Ù† Ø§Ù„Ø±Ø­Ù„Ø© Ø§Ù„ÙŠÙˆÙ… ÙƒØ§Ù†Øª Ù…Ø¯Ù‡Ø´Ø© âœ¨",
            "Ø­Ø¯Ø« Ù…Ù‡Ù… ØªÙ‚Ù†ÙŠ â€” Ù‚Ø±ÙŠØ¨Ù‹Ø§ ØªØºØ·ÙŠØ© ÙƒØ§Ù…Ù„Ø©.",
            "Ù‡Ù„ Ø£Ø­Ø¯ Ø¬Ø±Ø¨ Ø§Ù„Ø£Ø¯Ø§Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©ØŸ Ø´Ø§Ø±ÙƒÙˆÙ†Ø§ Ø±Ø£ÙŠÙƒÙ… ğŸ‘‡",
            "Ø§Ù‚ØªØ¨Ø§Ø³ Ø§Ù„ÙŠÙˆÙ…: Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø¬Ù…Ø§Ø¹ÙŠ ÙŠØµÙ†Ø¹ Ø§Ù„Ù…Ø¹Ø¬Ø²Ø§Øª."
        ]

        # ØªÙˆØ²ÙŠØ¹ Ø¹Ù„Ù‰ 7 Ø£ÙŠØ§Ù… (ÙƒÙ„ ØªØºØ±ÙŠØ¯Ø© ØªÙØ¹Ø·Ù‰ ÙŠÙˆÙ… Ù…Ø®ØªÙ„Ù ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹)
        days_back = list(range(7))  # 0 = Ø§Ù„ÙŠÙˆÙ…, 6 = Ù‚Ø¨Ù„ Ø£Ø³Ø¨ÙˆØ¹

        fake_tweets = []
        for i in range(10):
            # Ø§Ø®ØªØ± ÙŠÙˆÙ… Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù…Ù† Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ ÙˆØ³Ø§Ø¹Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
            day_offset = random.choice(days_back)
            created = now - timedelta(days=day_offset, hours=random.randint(0, 23))

            # Ø£Ø¶Ù ØµÙˆØ±Ø© Ù„Ø¨Ø¹Ø¶ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª
            media = []
            if i % 3 == 0:
                media.append(f"https://picsum.photos/seed/demo{i}/800/450")

            fake_tweets.append({
                "id": f"fake_{i+1}",
                "text": samples[i % len(samples)],
                "created_at": created.isoformat(),
                "public_metrics": {
                    "like_count": random.randint(5, 500),
                    "retweet_count": random.randint(0, 120),
                    "reply_count": random.randint(0, 80),
                    "impression_count": random.randint(200, 3000),
                },
                "media_urls": media
            })

        # Ø®Ø²Ù‘Ù† ÙˆØ¶Ø¹ Ø§Ù„ØªØ¬Ø±Ø¨Ø© ÙÙŠ Ø§Ù„Ø¬Ù„Ø³Ø©
        st.session_state["tweets_demo"] = fake_tweets
        st.session_state["username"] = "demo_user"
        st.session_state["user_id"] = "demo_id"
        st.session_state["access_token"] = "demo_token"  # Ù„ØªØ¬Ø§ÙˆØ² Ø´Ø±Ø· Ø§Ù„ØªÙˆÙƒÙ†
        st.success("âœ… ØªÙ… ØªÙØ¹ÙŠÙ„ ÙˆØ¶Ø¹ Ø§Ù„ØªØ¬Ø±Ø¨Ø©")

    st.markdown('</div>', unsafe_allow_html=True)


    # ====== Ø¨Ø¹Ø¯ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ù…Ù† ØªÙˆÙŠØªØ± (OAuth callback) ======
    q = st.query_params
    code, state = q.get("code"), q.get("state")
    if isinstance(code, list): code = code[0]
    if isinstance(state, list): state = state[0]

    if code and state and "access_token" not in st.session_state:
        code_verifier = load_state_from_db(state)
        if not code_verifier:
            st.error("âš ï¸ state ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ. Ø£Ø¹Ø¯ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©.")
            st.stop()

        token_url = "https://api.twitter.com/2/oauth2/token"
        data = {
            "grant_type": "authorization_code",
            "client_id": CLIENT_ID,
            "redirect_uri": REDIRECT_URI,
            "code": code,
            "code_verifier": code_verifier,
        }
        headers = {"Content-Type": "application/x-www-form-urlencoded"}
        r = requests.post(token_url, data=data, headers=headers, timeout=30)
        tokens = r.json()

        if "access_token" in tokens:
            st.session_state["access_token"] = tokens["access_token"]

            me = requests.get(
                "https://api.twitter.com/2/users/me",
                headers={"Authorization": f"Bearer {tokens['access_token']}"},
                timeout=30
            )
            user_data = me.json().get("data", {})
            st.session_state["user_id"] = user_data.get("id")
            st.session_state["username"] = user_data.get("username")

            # Ø®Ø²Ù‘Ù† expires_at/refresh_token ÙÙŠ Ø§Ù„Ø¬Ù„Ø³Ø© ÙˆØ§Ù„Ø¯Ø§ØªØ§Ø¨ÙŠØ³
            if "expires_in" in tokens:
                st.session_state["expires_at"] = (datetime.now() + timedelta(seconds=int(tokens["expires_in"]))).isoformat()
            if "refresh_token" in tokens:
                st.session_state["refresh_token"] = tokens["refresh_token"]

            save_tokens_to_db(
                st.session_state["user_id"],
                st.session_state["username"],
                {
                    "access_token": tokens["access_token"],
                    "refresh_token": tokens.get("refresh_token"),
                    "expires_in": tokens.get("expires_in")
                }
            )

            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±Ø§Ø¨Ø· (Ø¥Ø²Ø§Ù„Ø© query params)
            st.markdown(
                """
                <script>
                if (window.location.search.length > 0) {
                    window.history.replaceState({}, document.title, window.location.pathname);
                }
                </script>
                """,
                unsafe_allow_html=True
            )
        else:
            st.warning(" Ø­Ø§ÙˆÙ„ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰")


ensure_valid_token()

# ====== Ø¨Ø¹Ø¯ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ ======
USERNAME = st.session_state.get("username")
BEARER_TOKEN = st.session_state.get("access_token")

if not BEARER_TOKEN:
    st.stop()

#if not USERNAME:
#    st.error("ğŸš¨ Ù„Ù… Ù†Ø³ØªØ·Ø¹ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…. Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„Ø³Ø¨Ø¨ ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ Ø§Ù„Ù…Ø³Ù…ÙˆØ­ (429 Too Many Requests).")
 #   st.info("â³ Ø¬Ø±Ù‘Ø¨ Ø¨Ø¹Ø¯ ÙØªØ±Ø© Ø£Ùˆ Ø¨Ø¹Ø¯ Ø´Ù‡Ø±")
 #   st.stop()









# ====== Ø¯ÙˆØ§Ù„ API ======
def auth_header():
    return {"Authorization": f"Bearer {st.session_state.get('access_token')}"}


def get_user_id(username: str):
    url = f"https://api.twitter.com/2/users/by/username/{username}"
    r = requests.get(url, headers=auth_header(), timeout=30)
    if r.status_code == 401 and st.session_state.get("refresh_token"):
        rr = refresh_access_token(st.session_state["refresh_token"])
        if rr.status_code == 200:
            new_tokens = rr.json()
            st.session_state["access_token"] = new_tokens["access_token"]
            if "expires_in" in new_tokens:
                st.session_state["expires_at"] = (datetime.now() + timedelta(seconds=int(new_tokens["expires_in"]))).isoformat()
            # Ø®Ø²Ù‘Ù†
            if st.session_state.get("user_id") and st.session_state.get("username"):
                save_tokens_to_db(st.session_state["user_id"], st.session_state["username"], {
                    "access_token": new_tokens["access_token"],
                    "refresh_token": new_tokens.get("refresh_token", st.session_state.get("refresh_token")),
                    "expires_in": new_tokens.get("expires_in")
                })
            # Ø£Ø¹Ø¯ Ø§Ù„Ø·Ù„Ø¨
            r = requests.get(url, headers=auth_header(), timeout=30)
    r.raise_for_status()
    return r.json()["data"]["id"]


def get_latest_tweets(user_id: str, max_results: int = 80):
    url = f"https://api.twitter.com/2/users/{user_id}/tweets"
    params = {
        "tweet.fields": "public_metrics,created_at,attachments,referenced_tweets,entities",
        "expansions": "attachments.media_keys",
        "media.fields": "url,preview_image_url,type",
        "max_results": min(max_results, 100)
    }
    all_tweets = {}
    next_token = None

    while len(all_tweets) < max_results:
        if next_token:
            params["pagination_token"] = next_token
        elif "pagination_token" in params:
            params.pop("pagination_token")

        r = requests.get(url, headers=auth_header(), params=params, timeout=30)
        r.raise_for_status()
        data = r.json()

        tweets = data.get("data", [])
        if not tweets:
            break

        includes_media = {m["media_key"]: m for m in data.get("includes", {}).get("media", [])} if data.get("includes") else {}

        for t in tweets:
            media_urls = []

            # ğŸŸ¢ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø±Ø³Ù…ÙŠØ©: attachments.media_keys
            atts = t.get("attachments", {})
            if isinstance(atts, dict) and "media_keys" in atts:
                for mk in atts["media_keys"]:
                    m = includes_media.get(mk)
                    if not m:
                        continue
                    if m.get("type") == "photo" and m.get("url"):
                        media_urls.append(m["url"])
                    elif m.get("type") in ["video", "animated_gif"] and m.get("preview_image_url"):
                        media_urls.append(m["preview_image_url"])

            # ğŸŸ¡ fallback: Ù„Ùˆ Ù…Ø§ ÙÙŠÙ‡ media_keys â†’ Ø¬Ø±Ù‘Ø¨ entities.urls
            if not media_urls:
                for u in t.get("entities", {}).get("urls", []):
                    expanded = u.get("expanded_url")
                    if expanded and "pbs.twimg.com" in expanded:  # Ø³ÙŠØ±ÙØ± ØµÙˆØ± ØªÙˆÙŠØªØ±
                        media_urls.append(expanded)

            t["media_urls"] = media_urls
            all_tweets[t["id"]] = t

        next_token = data.get("meta", {}).get("next_token")
        if not next_token:
            break

        # ğŸ’¤ ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ· Ù‚Ø¨Ù„ Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„Ø¬Ø§ÙŠ
        time.sleep(2)

    return sorted(all_tweets.values(), key=lambda t: t.get("created_at", ""), reverse=True)[:max_results]



def save_tweets_to_db(tweets, user_id, username):
    conn = sqlite3.connect("tweets.db")
    c = conn.cursor()
    for t in tweets:
        pm = t.get("public_metrics", {}) or {}
        media_json = json.dumps(t.get("media_urls", []), ensure_ascii=False)
        c.execute("""
        INSERT OR REPLACE INTO tweets 
        (id, user_id, username, text, created_at, like_count, retweet_count, reply_count, impression_count, media_urls)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
        t["id"],
        user_id,
        username,
        t.get("text",""),
        t.get("created_at",""),
        pm.get("like_count",0),
        pm.get("retweet_count",0),
        pm.get("reply_count",0),
        pm.get("impression_count",0),
        media_json
    ))

    conn.commit()
    conn.close()


def load_tweets_from_db(username):
    conn = sqlite3.connect("tweets.db")
    c = conn.cursor()

    # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©
    c.execute("PRAGMA table_info(tweets)")
    cols = [col[1] for col in c.fetchall()]
    has_media_col = "media_urls" in cols

    if has_media_col:
        c.execute("""
            SELECT id, text, created_at, like_count, retweet_count, reply_count, impression_count, media_urls
            FROM tweets WHERE username=? ORDER BY created_at DESC
        """, (username,))
    else:
        c.execute("""
            SELECT id, text, created_at, like_count, retweet_count, reply_count, impression_count
            FROM tweets WHERE username=? ORDER BY created_at DESC
        """, (username,))

    rows = c.fetchall()
    conn.close()

    tweets = []
    for r in rows:
        if has_media_col:
            media_list = []
            try:
                media_list = json.loads(r[7]) if r[7] else []
            except Exception:
                media_list = []
            tweets.append({
                "id": r[0],
                "text": r[1],
                "created_at": r[2],
                "public_metrics": {
                    "like_count": r[3],
                    "retweet_count": r[4],
                    "reply_count": r[5],
                    "impression_count": r[6],
                },
                "media_urls": media_list
            })
        else:
            tweets.append({
                "id": r[0],
                "text": r[1],
                "created_at": r[2],
                "public_metrics": {
                    "like_count": r[3],
                    "retweet_count": r[4],
                    "reply_count": r[5],
                    "impression_count": r[6],
                },
                "media_urls": []  # Ù…Ø§ ÙÙŠÙ‡ ØµÙˆØ± Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù‚Ø¯ÙŠÙ…
            })
    return tweets




# ====== Ø¬Ù„Ø¨ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª ======
if "tweets_demo" in st.session_state:
    # ğŸ‘€ ÙˆØ¶Ø¹ Ø§Ù„ØªØ¬Ø±Ø¨Ø©
    tweets = st.session_state["tweets_demo"]
    USERNAME = st.session_state.get("username", "demo_user")
    last_fetch_date = "â€” (ÙˆØ¶Ø¹ ØªØ¬Ø±ÙŠØ¨ÙŠ)"

    st.sidebar.success("âœ… Ø£Ù†Øª ØªØ³ØªØ®Ø¯Ù… ÙˆØ¶Ø¹ Ø§Ù„ØªØ¬Ø±Ø¨Ø© (Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙ‡Ù…ÙŠØ©)")
else:
    # ğŸ‘¤ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø¹Ø§Ø¯ÙŠ
    tweets = load_tweets_from_db(USERNAME)

    last_fetch_date = None

    current_user_id = st.session_state.get("user_id")
    last_fetch_date = get_last_fetch(current_user_id) if current_user_id else None


    # ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ØªØ­Ø¯ÙŠØ«
    can_update = False
    if not last_fetch_date:
        can_update = True
    else:
        try:
            last_dt = datetime.fromisoformat(last_fetch_date)
            if (datetime.now().date() - last_dt.date()).days >= 1:
                can_update = True
        except Exception:
            # Ù„Ùˆ Ø§Ù„ØªØ§Ø±ÙŠØ® Ù…Ø®Ø±Ø¨ØŒ Ø§Ø³Ù…Ø­ Ø¨Ø§Ù„ØªØ­Ø¯ÙŠØ« Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ø«Ù… Ø¨ÙŠØªØµÙ„Ø­
            can_update = True


    if can_update:
        if st.sidebar.button("ğŸ”„ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¢Ù†"):
            try:
                user_id = get_user_id(USERNAME)
                new_tweets = get_latest_tweets(user_id, max_results=90)
                save_tweets_to_db(new_tweets, user_id, USERNAME)

                conn = sqlite3.connect("tweets.db")
                c = conn.cursor()
                c.execute("""
                    UPDATE user_tokens
                    SET last_fetch=?
                    WHERE user_id=?
                """, (datetime.now().isoformat(), user_id))
                conn.commit()
                conn.close()

                tweets = load_tweets_from_db(USERNAME)
                st.success(f"âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ({len(new_tweets)} ØªØºØ±ÙŠØ¯Ø© Ø¬Ø¯ÙŠØ¯Ø©) â€” Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ø¢Ù† {len(tweets)}")
            except Exception as e:
                st.error(f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ø¯ÙŠØ«: {e}")
                st.stop()
    else:
        st.sidebar.info("â³ Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ« Ù…Ø­ÙÙˆØ¸. ğŸ”” ØªÙ‚Ø¯Ø± ØªØ­Ø¯Ø« Ø¨Ø¹Ø¯ Ù…Ø±ÙˆØ± 24 Ø³Ø§Ø¹Ø©.")

# ====== ØªØ­Ù‚Ù‚ Ù„Ùˆ Ù…Ø§ÙÙŠÙ‡ Ø¨ÙŠØ§Ù†Ø§Øª ======
if not tweets:
    if "tweets_demo" in st.session_state:
        st.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© (ØªØ£ÙƒØ¯ Ù…Ù† Ø²Ø± ğŸ‘€ ÙˆØ¶Ø¹ Ø§Ù„ØªØ¬Ø±Ø¨Ø©).")
    else:
        st.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯. Ø¬Ø±Ù‘Ø¨ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ùˆ Ø³Ø¬Ù„ Ø§Ù„Ø¯Ø®ÙˆÙ„.")
    st.stop()

# ğŸ”„ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ø±ÙŠØ® (Ù…Ø±Ø© ÙˆØ­Ø¯Ø© ÙÙ‚Ø·)
formatted = format_last_fetch(last_fetch_date)


# ====== Ø¹Ø±Ø¶ ÙÙŠ Ø§Ù„Ù€ sidebar ======
st.sidebar.header("ğŸ”‘ Ø­Ø³Ø§Ø¨Ùƒ")
st.sidebar.success(f"âœ… Ù…ØªØµÙ„ ÙƒÙ€ @{USERNAME}")

tweets_count = len(tweets) if 'tweets' in locals() else 0

st.sidebar.markdown("### ğŸ“Š Ø­Ø§Ù„Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
st.sidebar.info(f"""
- ğŸ“ Ø¹Ø¯Ø¯ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª: **{tweets_count}**
- ğŸ—“ï¸ Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«: **{formatted}**
""")


# ====== ØªØ¬Ù‡ÙŠØ² DataFrame ======
def build_dataframe(raw_tweets):
    rows = []
    for t in raw_tweets:
        pm = t.get("public_metrics", {}) or {}
        created_dt = datetime.fromisoformat(t["created_at"].replace("Z","+00:00")) if t.get("created_at") else None
        created_str = created_dt.strftime("%Y-%m-%d %H:%M") if created_dt else ""
        is_reply = False
        if str(t.get("text","")).strip().startswith("@"):
            is_reply = True
        for ref in t.get("referenced_tweets",[]) or []:
            if ref.get("type")=="replied_to":
                is_reply = True
                break
        media_urls = t.get("media_urls",[]) or []
        rows.append({
            "id": t.get("id",""),
            "Ø§Ù„Ù†Øµ": t.get("text",""),
            "ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±": created_str,
            "DT": created_dt,
            "Ø§Ù„Ø¥Ø¹Ø¬Ø§Ø¨Ø§Øª": pm.get("like_count",0),
            "Ø§Ù„Ø±ÙŠØªÙˆÙŠØª": pm.get("retweet_count",0),
            "Ø§Ù„Ø±Ø¯ÙˆØ¯": pm.get("reply_count",0),
            "Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª": pm.get("impression_count",0),
            "Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„": pm.get("like_count",0)+pm.get("retweet_count",0)+pm.get("reply_count",0),
            "Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ (%)": 0.0,
            "has_media": len(media_urls)>0,
            "media_urls": media_urls,
            "is_reply": is_reply
        })
    df_ = pd.DataFrame(rows)
    df_["Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ (%)"] = np.where(
        df_["Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª"]>0,
        (df_["Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"]/df_["Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª"]*100).round(2),
        0.0
    )
    return df_

df = build_dataframe(tweets)

# ====== ØªØ¨ÙˆÙŠØ¨Ø§Øª Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© ======
tab1, tab2 = st.tabs(["ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙØ§Ø¹Ù„", "ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø³Ø§Ø¨"])

# =========================================================
#                       Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„
# =========================================================
with tab1:
    st.title("ğŸ“Š Ù„ÙˆØ­Ø© ØªØ­ÙƒÙ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª")

    # --- ÙÙ„Ø§ØªØ± Ø§Ù„Ø¹Ø±Ø¶ ---
    st.subheader("ğŸ” ÙÙ„Ø§ØªØ±")
    col_f1, col_f2, col_f3 = st.columns(3)
    with col_f1:
        keyword = st.text_input("Ø§Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø© (Ù†Øµ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©)")
    with col_f2:
        date_min = st.date_input("Ù…Ù† ØªØ§Ø±ÙŠØ®", value=(df["DT"].min().date() if df["DT"].notna().any() else datetime.now().date()))
    with col_f3:
        date_max = st.date_input("Ø¥Ù„Ù‰ ØªØ§Ø±ÙŠØ®", value=(df["DT"].max().date() if df["DT"].notna().any() else datetime.now().date()))

    col_f4, col_f5, col_f6 = st.columns(3)
    with col_f4:
        max_eng = int(df["Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"].max() or 0)
        if max_eng == 0:
            max_eng = 1  # Ø¹Ø´Ø§Ù† Ù…Ø§ ÙŠØµÙŠØ± min=max

        min_eng = st.slider(
            "Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„",
            0,
            max_eng,
            0
        )

    with col_f5:
        kind = st.selectbox("Ù†ÙˆØ¹ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©", ["Ø§Ù„ÙƒÙ„", "ØªØºØ±ÙŠØ¯Ø§Øª Ø£ØµÙ„ÙŠØ© ÙÙ‚Ø·", "Ù…Ù†Ø´Ù† ÙÙ‚Ø·"])
    with col_f6:
        only_media = st.checkbox("ğŸ“· Ø¹Ø±Ø¶ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ ÙˆØ³Ø§Ø¦Ø· ÙÙ‚Ø·", key="filter_media_checkbox")
        only_charts = st.checkbox(
        "ğŸ“Š Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø³ÙˆÙ… ÙÙ‚Ø·",
        value=False,
        help="Ø¥Ø°Ø§ ÙØ¹Ù„Øª Ù‡Ø°Ø§ Ø§Ù„Ø®ÙŠØ§Ø± Ø³ÙŠØªÙ… Ø¥Ø®ÙØ§Ø¡ Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª"
    )
    
    

    # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ±
    filtered = df.copy()
    filtered = filtered[(filtered["DT"].dt.date >= date_min) & (filtered["DT"].dt.date <= date_max)]
    if keyword:
        filtered = filtered[filtered["Ø§Ù„Ù†Øµ"].str.contains(keyword, case=False, na=False)]
    if min_eng > 0:
        filtered = filtered[filtered["Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"] >= min_eng]
    if kind == "ØªØºØ±ÙŠØ¯Ø§Øª Ø£ØµÙ„ÙŠØ© ÙÙ‚Ø·":
        filtered = filtered[filtered["is_reply"] == False]
    elif kind == "Ù…Ù†Ø´Ù† ÙÙ‚Ø·":
        filtered = filtered[filtered["is_reply"] == True]
    if only_media:
        filtered = filtered[filtered["has_media"] == True]

    st.write(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ **{len(filtered)}** ØªØºØ±ÙŠØ¯Ø© Ù…Ø·Ø§Ø¨Ù‚Ø©")

    if filtered.empty:
        st.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ ØªØºØ±ÙŠØ¯Ø§Øª Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¨Ø¹Ø¯ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ±.")
        st.stop()

    # --- Ù…Ø¤Ø´Ø± Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ø¹Ø§Ù… ---
    st.subheader("ğŸ“ˆ Ù…Ø¤Ø´Ø± Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ø¹Ø§Ù… (Engagement Index)")
    st.caption("Ù…Ø¤Ø´Ø± Ø³Ø±ÙŠØ¹: Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„ Ã· Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª Ø¹Ø¨Ø± Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ù…ÙÙ„ØªØ±Ø©.")
    total_eng = int(filtered["Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"].sum())
    total_impr = int(filtered["Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª"].sum())
    ei = (total_eng / total_impr * 100) if total_impr > 0 else 0
    c1, c2, c3 = st.columns(3)
    c1.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„", f"{total_eng:,}")
    c2.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª", f"{total_impr:,}")
    c3.metric("Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„ÙƒÙ„ÙŠØ©", f"{ei:.2f}%")

    # --- Ø¹Ø±Ø¶ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª ÙƒØ¨Ø·Ø§Ù‚Ø§Øª ---
    st.subheader("ğŸ“ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª")
    st.caption("ÙƒÙ„ Ø¨Ø·Ø§Ù‚Ø© ØªØ¹Ø±Ø¶ Ø£Ù‡Ù… Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ØªØºØ±ÙŠØ¯Ø© Ù…Ø¹ Ø±Ø§Ø¨Ø· Ù…Ø¨Ø§Ø´Ø± ÙˆØµÙˆØ± Ø¥Ù† ÙˆØ¬Ø¯Øª.")

    # --- Ø¹Ø±Ø¶ Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª ---
    if not only_charts:
        for _, row in filtered.iterrows():
            tweet_url = f"https://twitter.com/{USERNAME}/status/{row['id']}"
            st.markdown(
                f"""
                <div style='direction: rtl; text-align: right; border:1px solid #444; border-radius:10px; padding:10px; margin-bottom:8px;'>
                <b>Ø§Ù„Ù†Øµ:</b> {row['Ø§Ù„Ù†Øµ']}<br>
                <b>ğŸ“… ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±:</b> {row['ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±']}<br>
                â¤ï¸ <b>{row['Ø§Ù„Ø¥Ø¹Ø¬Ø§Ø¨Ø§Øª']}</b> | ğŸ” <b>{row['Ø§Ù„Ø±ÙŠØªÙˆÙŠØª']}</b> | ğŸ’¬ <b>{row['Ø§Ù„Ø±Ø¯ÙˆØ¯']}</b> |
                ğŸ‘€ <b>{row['Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª']}</b> | ğŸ“Š <b>Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {row['Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„']}</b> | % <b>{row['Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ (%)']}</b><br>
                ğŸ”— <a href="{tweet_url}" target="_blank">ÙØªØ­ Ø§Ù„ØªØºØ±ÙŠØ¯Ø© Ø¹Ù„Ù‰ X</a>
                </div>
                """,
                unsafe_allow_html=True
            )
            if row["has_media"]:
                for m in row["media_urls"]:
                    st.image(m, use_container_width=True)



    else:
        st.info("ğŸ—‚ï¸ ØªÙ… Ø¥Ø®ÙØ§Ø¡ Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ù„Ø§Ù†Ùƒ Ù…ÙØ¹Ù„ Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø³ÙˆÙ… ÙÙ‚Ø·")

    # âœ… Ø§Ù„Ø±Ø³ÙˆÙ… Ø£ØµØ¨Ø­Øª Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¸Ø§Ù‡Ø±Ø© Ø¨ØºØ¶ Ø§Ù„Ù†Ø¸Ø± Ø¹Ù† show_cards

    st.subheader("ğŸ”¥ Ø£ÙƒØ«Ø± Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª ØªÙØ§Ø¹Ù„Ù‹Ø§")
    st.caption("Ø£ÙØ¶Ù„ 10 ØªØºØ±ÙŠØ¯Ø§Øª Ø­Ø³Ø¨ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„.")
    top10 = filtered.sort_values(by="Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„", ascending=False).head(10)
    if len(top10) > 0:
        fig, ax = plt.subplots()
        ax.barh([reshape_label(str(t)[:50]) for t in top10["Ø§Ù„Ù†Øµ"]], top10["Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"])
        ax.set_xlabel(reshape_label("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„")); ax.set_ylabel(reshape_label("Ø§Ù„Ù†Øµ"))
        beautify_axes(ax)
        st.pyplot(fig)
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ ØªØºØ±ÙŠØ¯Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„Ø±Ø³Ù….")

    st.subheader("â° Ø£ÙØ¶Ù„ Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ù†Ø´Ø± (Ù…ØªÙˆØ³Ø· Ø§Ù„ØªÙØ§Ø¹Ù„)")
    st.caption("Ù…ØªÙˆØ³Ø· Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„ Ù„ÙƒÙ„ Ø³Ø§Ø¹Ø© Ù†Ø´Ø±.")
    if filtered["DT"].notna().any():
        filtered["Ø³Ø§Ø¹Ø©"] = filtered["DT"].dt.hour
        hourly = filtered.groupby("Ø³Ø§Ø¹Ø©")["Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"].mean()
        fig2, ax2 = plt.subplots()
        hourly.plot(kind="bar", ax=ax2)
        ax2.set_xlabel(reshape_label("Ø³Ø§Ø¹Ø© Ø§Ù„Ù†Ø´Ø±")); ax2.set_ylabel(reshape_label("Ù…ØªÙˆØ³Ø· Ø§Ù„ØªÙØ§Ø¹Ù„"))
        beautify_axes(ax2)
        st.pyplot(fig2)
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙˆØ§Ø±ÙŠØ® ØµØ§Ù„Ø­Ø©.")

    st.subheader("ğŸ“ˆ Ø£Ø¹Ù„Ù‰ Ù†Ø³Ø¨ Ø§Ù„ØªÙØ§Ø¹Ù„")
    st.caption("Ø£ÙØ¶Ù„ 10 ØªØºØ±ÙŠØ¯Ø§Øª Ø­Ø³Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ (Ø§Ù„ØªÙØ§Ø¹Ù„ Ã· Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª).")
    top_rate = filtered.sort_values(by="Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ (%)", ascending=False).head(10)
    if len(top_rate) > 0:
        fig3, ax3 = plt.subplots()
        ax3.barh([reshape_label(str(t)[:50]) for t in top_rate["Ø§Ù„Ù†Øµ"]], top_rate["Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ (%)"])
        ax3.set_xlabel(reshape_label("Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ (%)"))
        beautify_axes(ax3)
        st.pyplot(fig3)
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ©.")

    # --- Heatmap ---
    st.subheader("ğŸ“… Ø£ÙØ¶Ù„ Ø§Ù„Ø£ÙˆÙ‚Ø§Øª Ù„Ù„Ù†Ø´Ø± (Heatmap)")
    st.caption("Ù…ØªÙˆØ³Ø· Ø§Ù„ØªÙØ§Ø¹Ù„ Ø­Ø³Ø¨ Ø§Ù„ÙŠÙˆÙ… ÙˆØ§Ù„Ø³Ø§Ø¹Ø©.")
    tmp = filtered.copy()
    if not tmp.empty:
        if "ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±_DT" not in tmp.columns:
            tmp["ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±_DT"] = pd.to_datetime(tmp["ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±"], errors="coerce")
        tmp["Ø§Ù„ÙŠÙˆÙ…_Ø§Ù†Ø¬Ù„ÙŠØ²ÙŠ"] = tmp["ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±_DT"].dt.day_name()
        tmp["Ø§Ù„ÙŠÙˆÙ…"] = tmp["Ø§Ù„ÙŠÙˆÙ…_Ø§Ù†Ø¬Ù„ÙŠØ²ÙŠ"].map({
            "Sunday": "Ø§Ù„Ø£Ø­Ø¯", "Monday": "Ø§Ù„Ø§Ø«Ù†ÙŠÙ†", "Tuesday": "Ø§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡",
            "Wednesday": "Ø§Ù„Ø£Ø±Ø¨Ø¹Ø§Ø¡", "Thursday": "Ø§Ù„Ø®Ù…ÙŠØ³", "Friday": "Ø§Ù„Ø¬Ù…Ø¹Ø©", "Saturday": "Ø§Ù„Ø³Ø¨Øª"
        })
        tmp["Ø§Ù„Ø³Ø§Ø¹Ø©"] = tmp["ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±_DT"].dt.hour
        pivot_table = tmp.pivot_table(index="Ø§Ù„ÙŠÙˆÙ…", columns="Ø§Ù„Ø³Ø§Ø¹Ø©", values="Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„", aggfunc="mean", fill_value=0)
        if pivot_table.empty:
            st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ø¥Ù†Ø´Ø§Ø¡ Heatmap.")
        else:
            days_order = ["Ø§Ù„Ø£Ø­Ø¯","Ø§Ù„Ø§Ø«Ù†ÙŠÙ†","Ø§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡","Ø§Ù„Ø£Ø±Ø¨Ø¹Ø§Ø¡","Ø§Ù„Ø®Ù…ÙŠØ³","Ø§Ù„Ø¬Ù…Ø¹Ø©","Ø§Ù„Ø³Ø¨Øª"]
            pivot_table = pivot_table.reindex(days_order)
            fig_hm, ax_hm = plt.subplots(figsize=(10, 5))
            cax = ax_hm.imshow(pivot_table, cmap="YlOrRd", aspect="auto")
            ax_hm.set_yticks(range(len(pivot_table.index)))
            ax_hm.set_yticklabels([reshape_label(day) for day in pivot_table.index])
            ax_hm.set_xticks(range(len(pivot_table.columns)))
            ax_hm.set_xticklabels([reshape_label(str(col)) for col in pivot_table.columns], rotation=90)
            ax_hm.set_xlabel(reshape_label("Ø§Ù„Ø³Ø§Ø¹Ø©")); ax_hm.set_ylabel(reshape_label("Ø§Ù„ÙŠÙˆÙ…"))
            ax_hm.set_title(reshape_label("Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªÙØ§Ø¹Ù„ Ø­Ø³Ø¨ Ø§Ù„ÙŠÙˆÙ… ÙˆØ§Ù„Ø³Ø§Ø¹Ø©"))
            fig_hm.colorbar(cax, ax=ax_hm, label=reshape_label("Ù…ØªÙˆØ³Ø· Ø§Ù„ØªÙØ§Ø¹Ù„"))
            beautify_axes(ax_hm)
            st.pyplot(fig_hm)
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ©.")

    # --- ØªÙˆÙ‚Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ ---
    st.subheader("ğŸ”® ØªÙˆÙ‚Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ")
    st.caption("Ø¹Ù„Ø§Ù‚Ø© Ø®Ø·ÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª ÙˆØ§Ù„ØªÙØ§Ø¹Ù„ Ù„ØªÙ‚Ø¯ÙŠØ± Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹.")
    if len(filtered) >= 4 and filtered["Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª"].sum() > 0:
        X = np.array(filtered["Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª"]).reshape(-1, 1)
        y = np.array(filtered["Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"])
        try:
            model = LinearRegression().fit(X, y)
            st.info(f"Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø±: {model.coef_[0]:.4f} | Ø§Ù„Ø«Ø§Ø¨Øª: {model.intercept_:.2f}")
            future_impr = st.number_input("Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©:", min_value=0, value=500, step=50)
            pred = model.predict([[future_impr]])[0]
            st.success(f"Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {pred:.0f}")

            fig_lr, ax_lr = plt.subplots()
            ax_lr.scatter(X, y, label=reshape_label("Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª"))
            x_line = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
            ax_lr.plot(x_line, model.predict(x_line), label=reshape_label("Ø®Ø· Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø±"))
            ax_lr.set_xlabel(reshape_label("Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª")); ax_lr.set_ylabel(reshape_label("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"))
            ax_lr.legend()
            beautify_axes(ax_lr)
            st.pyplot(fig_lr)
        except Exception as e:
            st.warning(f"ØªØ¹Ø°Ù‘Ø± ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø±: {e}")
    else:
        st.info("ØªØ­ØªØ§Ø¬ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ 4 ØªØºØ±ÙŠØ¯Ø§Øª Ø°Ø§Øª Ù…Ø´Ø§Ù‡Ø¯Ø§Øª > 0 Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.")

    # --- Ø³Ø­Ø§Ø¨Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª ---
    st.subheader("â˜ï¸ Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‹Ø§")
    st.caption("ÙŠØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆØ§Ù„Ù…Ù†Ø´Ù† ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙˆÙ‚ÙÙŠØ© Ù‚Ø¨Ù„ Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±.")
    word_counts = {}
    if not filtered.empty:
        for _, row in filtered.iterrows():
            text = re.sub(r"http\S+|www\S+|@\S+", "", str(row["Ø§Ù„Ù†Øµ"]))
            words = text.split()
            for w in words:
                w = w.strip().lower()
                if w.startswith("Ø§Ù„"):
                    w = w[2:]
                if w in {"ÙÙŠ","Ø¹Ù„Ù‰","Ù…Ù†","Ø¹Ù†","Ø§Ù„Ù‰","Ø¥Ù„Ù‰","Ùˆ","Ø§Ùˆ","Ø£Ùˆ","Ù…Ø§","Ù„Ø§","Ù‡Ø°Ø§","Ù‡Ø°Ù‡","Ø°Ù„Ùƒ","Ù‡Ø°ÙŠ","Ù‡Ø°ÙŠÙƒ"}:
                    continue
                if w:
                    word_counts[reshape_label(w)] = word_counts.get(reshape_label(w), 0) + 1

    if word_counts:
        try:
            wordcloud = WordCloud(font_path="arial.ttf", width=1200, height=600,
                                background_color="white", max_words=100, min_font_size=14,
                                colormap="plasma").generate_from_frequencies(word_counts)
            fig_wc, ax_wc = plt.subplots(figsize=(12, 6))
            ax_wc.imshow(wordcloud, interpolation="bilinear"); ax_wc.axis("off")
            st.pyplot(fig_wc)
        except Exception:
            st.info("ØªØ¹Ø°Ù‘Ø± ØªÙˆÙ„ÙŠØ¯ Ø³Ø­Ø§Ø¨Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª (ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø®Ø· Ø¹Ø±Ø¨ÙŠ Ù…Ø«Ù„ arial.ttf).")

        st.markdown("### ğŸ† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹")
        top_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:20]
        top_df = pd.DataFrame(top_words, columns=["Ø§Ù„ÙƒÙ„Ù…Ø©", "Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª"])
        top_df["Ø§Ù„ÙƒÙ„Ù…Ø©"] = top_df["Ø§Ù„ÙƒÙ„Ù…Ø©"].apply(reshape_label)
        st.markdown(top_df.to_html(index=False, justify="right"), unsafe_allow_html=True)
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ ÙƒÙ„Ù…Ø§Øª ÙƒØ§ÙÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ.")

   

# =========================================================
#                       ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø³Ø§Ø¨
# =========================================================
with tab2:
    st.title("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø´Ø§Ù…Ù„")
    st.info("Ù‡Ø°Ù‡ Ø§Ù„ØªØ¨ÙˆÙŠØ¨Ø© ØªØ¹Ø±Ø¶ ØªØ­Ù„ÙŠÙ„Ø§Øª Ù…Ø¹Ù…Ù‘Ù‚Ø©: Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„ÙƒØªØ§Ø¨Ø©ØŒ Ø§Ù„Ù‡Ø§Ø´ØªØ§Ù‚Ø§ØªØŒ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·ØŒ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·ØŒ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±ØŒ ÙˆØ§Ù„Ù€ n-grams.")

    # --- ØªØ¬Ù‡ÙŠØ² DF Ù…Ø®ØµØµ Ù„Ù„ØªØ­Ù„ÙŠÙ„ (Ù…Ø¨Ù†ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙÙ„Ø§ØªØ± Ù…Ù† Ø§Ù„ØªØ§Ø¨ 1) ---
    @st.cache_data
    def prepare_profile_dataframe(tweets_local):
        rows = []
        for t in tweets_local:
            created_dt = datetime.fromisoformat(t["created_at"].replace("Z", "+00:00")) if t.get("created_at") else None
            created_str = created_dt.strftime("%Y-%m-%d %H:%M") if created_dt else ""
            txt = str(t.get("text", ""))

            rows.append({
                "id": t.get("id", ""),
                "Ø§Ù„Ù†Øµ": txt,
                "ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±": created_str,
                "DT": created_dt,
                "hashtags": re.findall(r"#(\w+)", txt),
                "mentions": re.findall(r"@(\w+)", txt),
                "contains_link": bool(re.search(r"http\S+", txt)),
                "links": re.findall(r"http[s]?://[^\s]+", txt),
                "is_reply": str(txt).strip().startswith("@"),
                "has_media": len(t.get("media_urls", []) or []) > 0,
                "media_urls": t.get("media_urls", []) or []
            })
        return pd.DataFrame(rows)

    # âœ… Ø§Ø³ØªØ®Ø¯Ù… ÙÙ‚Ø· Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø§Ù„Ù…ÙÙ„ØªØ±Ø©
    filtered_ids = set(filtered["id"].astype(str))
    tweets_filtered_raw = [t for t in tweets if str(t.get("id", "")) in filtered_ids]
    pdf = prepare_profile_dataframe(tweets_filtered_raw)

    # ğŸ”’ Ø£Ø¶Ù ÙØ­Øµ Ù‡Ù†Ø§ Ù‚Ø¨Ù„ Ø£ÙŠ Ø±Ø³Ù… Ø£Ùˆ ØªØ­Ù„ÙŠÙ„
    if pdf.empty:
        st.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¨Ø¹Ø¯ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ± Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª.")
        st.stop()

    
    # =============== ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± =============== 
        
    # --- Ù‚Ø§Ù…ÙˆØ³ Ù…Ø´Ø§Ø¹Ø± Ø¹Ø±Ø¨ÙŠ Ù…ÙˆØ³Ù‘Ø¹ Ù…Ø¹ ÙƒÙ„Ù…Ø§Øª Ø¹Ø§Ù…ÙŠØ© ---
    AR_POS = {
        # Ø±Ø³Ù…ÙŠ + ÙØµÙŠØ­
        "Ù…Ù…ØªØ§Ø²","Ø¬Ù…ÙŠÙ„","Ø±Ø§Ø¦Ø¹","Ø£ÙØ¶Ù„","Ù…Ù…ÙŠØ²","Ø´ÙƒØ±Ø§","Ø³Ø¹ÙŠØ¯","Ù…Ø­Ø¨ÙˆØ¨","Ù†Ø¬Ø§Ø­","Ø­Ù„Ùˆ","ÙØ§Ø®Ø±","Ù‚ÙˆÙŠ","Ù…ÙÙŠØ¯",
        "Ù…Ø¨Ù‡Ø¬","Ù…ÙØ±Ø­","Ù…Ø³Ø±ÙˆØ±","Ù…Ø±ÙŠØ­","Ù„Ø·ÙŠÙ","Ù…Ø·Ù…Ø¦Ù†","Ù…Ø¨Ù‡Ø±","Ù…Ø°Ù‡Ù„","Ø¹Ø¸ÙŠÙ…","Ø³ÙˆØ¨Ø±","Ø¹Ø¨Ù‚Ø±ÙŠ","Ù…Ø«Ø§Ù„ÙŠ",
        "Ù…Ø­ØªØ±Ù…","Ø±Ø§Ù‚ÙŠ","Ø¨Ø·Ù„","Ù…ÙƒØ³Ø¨","Ù…ØªÙÙˆÙ‚","Ù‡ÙŠØ¨Ø©","ÙØ±Ø­Ø©","Ù…Ø±Ø­","Ù…Ø¨Ø±ÙˆÙƒ","Ù…ÙˆÙÙ‚","Ø±Ø§Ø¨Ø­","Ù…Ø±Ø¨Ø­",
        "Ø³Ø§Ø­Ø±","Ù…Ø´Ø±Ù‚","Ù…Ù…ÙŠØ²","Ù‚ÙˆÙŠ","Ø´Ø¬Ø§Ø¹","Ø¥ÙŠØ¬Ø§Ø¨ÙŠ","Ù…Ø±ÙŠØ­","Ø¬Ù…ÙŠÙ„","Ù…ØªØ­Ù…Ø³","Ù…Ø³Ø±Ø­ÙŠ","Ø³Ù„Ø§Ù…","Ø­Ø¨","Ø®ÙŠØ±",
        "ÙˆØ§Ùˆ","Ù…Ø°Ù‡ÙˆÙˆÙ„","ÙÙ„","Ø®Ø±Ø§ÙÙŠ","Ø§Ø³Ø·ÙˆØ±ÙŠ","Ø®ÙˆØ±Ø§ÙÙŠ","Ø¬Ù†Ø§Ù†","Ø±ÙˆØ¹Ø©","ØªØ­ÙØ©","Ø®ÙŠØ§Ù„","Ù‚Ù†Ø¨Ù„Ø©","ÙØ§ÙŠÙ_Ø³ØªØ§Ø±",
        "Ø¬Ø§Ù…Ø¯","ÙƒÙÙˆ","Ø³Ø·ÙˆØ±ÙŠ","ÙÙ†Ø§Ù†","Ø¨Ø·Ù„","Ø­Ù…Ø§Ø³","ÙÙ„Ø©","Ù…Ø²ÙŠØ§Ù†","Ø¬ÙŠØ¯","Ø±Ø§ÙŠÙ‚","Ø¹Ø§Ù„Ù…ÙŠ","ØªÙˆØ¨","Ø³ÙˆØ¨Ø±","Ù‚ÙˆÙŠÙŠÙŠ",
        "ÙÙ„Ù‘Ø©","Ù‚Ù„Ø¨","ÙƒÙˆÙŠØ³","Ø²ÙŠÙ†","Ø·Ø±Ø¨","Ø·Ù‚Ø·Ù‚Ù‡ Ø­Ù„ÙˆÙ‡","ÙÙ†Ø§Ù†","Ù…Ù…ØªØ§Ø²Ø©","Ù…Ø±ØªØ¨","Ø±Ù‡ÙŠØ¨","Ø´ÙŠÙƒ","Ù…Ù‡ÙŠØ¨","Ø¹Ø¬Ø¨Ù†ÙŠ"
    }

    AR_NEG = {
        # Ø±Ø³Ù…ÙŠ + ÙØµÙŠØ­
        "Ø³ÙŠØ¡","Ø³Ø¦","Ø±Ø¯ÙŠØ¡","Ø£Ø³ÙˆØ£","Ø­Ø²ÙŠÙ†","Ø®Ø³Ø§Ø±Ø©","ÙØ´Ù„","Ù…Ø²ÙŠÙ","Ø¶Ø¹ÙŠÙ","Ø²Ø¹Ø¬","Ø¥Ø²Ø¹Ø§Ø¬","ÙƒØ§Ø±Ø«ÙŠ","ØºÙ„Ø·",
        "Ù…Ø²Ø¹Ø¬","Ù…ØªØ¹Ø¨","Ù…Ù‚Ø±Ù","Ù…Ù…Ù„","Ù…Ø¤Ù„Ù…","Ù…Ø­Ø¨Ø·","Ø¨Ø´Ø¹","Ø­Ù‚Ø¯","ÙƒØ±Ø§Ù‡ÙŠØ©","Ø¹Ø¯Ø§Ø¡","Ù…Ø£Ø³Ø§Ø©","ÙƒØ§Ø±Ø«Ø©","Ø³Ù„Ø¨ÙŠØ©",
        "Ø¶ÙŠØ§Ø¹","Ù‚Ù„Ø©","ÙÙˆØ¶Ù‰","Ø¬Ø±Ø­","Ø®ÙˆÙ","Ù…ØµÙŠØ¨Ø©","ÙƒØ³Ø±","Ø¹Ù†Ù","Ø¸Ù„Ù…","Ù…Ø´ÙƒÙ„Ø©","Ø¶Ø¹Ù","Ù‡Ø²ÙŠÙ…Ø©","ÙƒØ¦ÙŠØ¨",
        "Ù…Ø¹Ø§Ù†Ø§Ø©","Ø¨Ø§Ø¦Ø³","Ù…Ù‚Ø²Ø²","Ø³Ù…","Ø£Ø°Ù‰","Ù…Ø±Ø¹Ø¨","ØºØ¶Ø¨","Ù„Ø¹Ù†Ø©","Ø¹Ø§Ø±","Ø®ÙŠØ¨Ø©","ÙØ¶ÙŠØ­Ø©","Ù…Ø£Ø³Ø§ÙˆÙŠ","Ù…Ø²Ø¹Ø¬Ø©",
        # Ø¹Ø§Ù…ÙŠØ© + ØªÙˆÙŠØªØ±
        "Ù‚Ù‡Ø±","Ø·ÙØ´","Ø®Ø§ÙŠØ³","Ø¨Ø§ÙŠØ®","Ø²ÙØª","Ø®Ø§ÙŠØµ","Ø´ÙŠÙ†","ØªØ¹Ø¨","Ù‡Ù…","Ù†ÙƒØ¯","Ù…Ù‚Ù„Ø¨","ØºØ¨Ø§Ø¡","Ø®Ø±Ø§Ø¨","ØªØ§ÙÙ‡",
        "Ù…Ø³Ø®Ø±Ø©","Ø¨Ù„Ø§Ø¡","Ù†Ø±ÙØ²Ø©","Ù‚Ø±Ù","ØªØ¹Ø¨Ø§Ù†","Ø·ÙØ´Ø§Ù†","ÙŠØ§ Ø³Ø§ØªØ±","Ø¨Ø±Ø¨Ø³Ø©","Ù…ØºØ¨ÙˆÙ†","Ù‚Ø­Ø·","Ù…Ù„Ù„","Ù…Ù„Ù„","Ø¯Ù…Ø§Ø±",
        "Ø·ÙŠØ­Ù†ÙŠ","ÙØ´Ù„Ù†ÙŠ","Ù…Ø·ÙÙˆÙ‚","Ù‚Ø±ÙØ§Ù†","Ø³Ù…Ø¬","ÙƒØ±ÙŠÙ‡","Ù†Ø±ÙØ²","Ø®Ø±Ø§","Ø²Ø¹Ù„Ø§Ù†","Ù…Ù†Ø­ÙˆØ³","Ù…Ø®ÙŠØ³","Ø®Ø§Ø²ÙˆÙ‚","Ø´Ù†ÙŠØ¹"
    }

    EN_POS = {
    "good","great","awesome","amazing","love","like","nice","happy","perfect","excellent","cool",
    "win","strong","helpful","fantastic","wonderful","brilliant","super","genius","epic","respect",
    "best","top","success","profit","joy","fun","wow","outstanding","positive","cheer","bless","safe",
    "bright","hero","masterpiece","congrats","peace","hope","gift","advantage","benefit","enjoy","smile",
    }

    EN_NEG = {
        "bad","worse","worst","sad","angry","hate","disappoint","fake","weak","annoy","terrible","awful",
        "fail","loss","mess","broken","pain","boring","useless","stupid","lazy","ugly","trouble","risk",
        "fear","danger","negative","cry","problem","miss","chaos","harm","toxic","slow","sucks","hate","mad",
        "angry","depress","scary","nightmare","down","damage","fraud","spam","junk","block","poor",
    }

    def normalize_ar(text: str) -> str:
        s = str(text)

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆØ§Ù„Ù…Ù†Ø´Ù† ÙˆØ§Ù„Ù‡Ø§Ø´ØªØ§Ù‚Ø§Øª
        s = re.sub(r"http\S+|www\S+|@\S+", " ", s)
        s = re.sub(r"#", " ", s)

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ ÙˆØ§Ù„Ù…Ø¯ÙˆØ¯
        s = re.sub(r"[\u0617-\u061A\u064B-\u0652\u0670\u06D6-\u06ED\u0640]", "", s)

        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù‡Ù…Ø²Ø§Øª
        s = s.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")

        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„ÙŠØ§Ø¡ ÙˆØ§Ù„Ø£Ù„Ù Ø§Ù„Ù…Ù‚ØµÙˆØ±Ø©
        s = s.replace("Ù‰", "ÙŠ")

        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù‡Ø§Ø¡ ÙˆØ§Ù„ØªØ§Ø¡ Ø§Ù„Ù…Ø±Ø¨ÙˆØ·Ø©
        s = s.replace("Ø©", "Ù‡")

        # Ø¥Ø²Ø§Ù„Ø© ØªÙƒØ±Ø§Ø± Ø§Ù„Ø­Ø±ÙˆÙ Ø£ÙƒØ«Ø± Ù…Ù† Ù…Ø±ØªÙŠÙ† (Ø±Ø§Ø§Ø§Ø§Ø§Ø¦Ø¹ â†’ Ø±Ø§Ø¦Ø¹)
        s = re.sub(r"(.)\1{2,}", r"\1", s)

        # Ù…Ø³Ø§ÙØ§Øª Ù†Ø¸ÙŠÙØ©
        s = re.sub(r"\s+", " ", s).strip().lower()
        return s


    def simple_sentiment(text: str) -> int:
        """ÙŠØ±Ø¬Ø¹: 1 Ø¥ÙŠØ¬Ø§Ø¨ÙŠØŒ -1 Ø³Ù„Ø¨ÙŠØŒ 0 Ù…Ø­Ø§ÙŠØ¯."""
        t = normalize_ar(text)
        toks = re.findall(r"[a-zA-Z\u0600-\u06FF]+", t)
        score = 0
        for w in toks:
            lw = w.lower()
            if lw in AR_POS or lw in EN_POS: score += 1
            if lw in AR_NEG or lw in EN_NEG: score -= 1
        if score > 0: return 1
        if score < 0: return -1
        return 0


    # =============== ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ø²Ù…Ù†ÙŠ ===============
    st.subheader("ğŸ•’ Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ø²Ù…Ù†ÙŠ")
    st.caption("ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø¹Ù„Ù‰ Ø£ÙŠØ§Ù… Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ ÙˆØ³Ø§Ø¹Ø§Øª Ø§Ù„ÙŠÙˆÙ… + Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØºØ±ÙŠØ¯ Ø§Ù„ÙŠÙˆÙ…ÙŠ.")
    if not pdf.empty:
        pdf["Ø§Ù„ÙŠÙˆÙ…"] = pdf["DT"].dt.day_name()
        day_map = {"Sunday":"Ø§Ù„Ø£Ø­Ø¯", "Monday":"Ø§Ù„Ø§Ø«Ù†ÙŠÙ†", "Tuesday":"Ø§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡",
                   "Wednesday":"Ø§Ù„Ø£Ø±Ø¨Ø¹Ø§Ø¡", "Thursday":"Ø§Ù„Ø®Ù…ÙŠØ³", "Friday":"Ø§Ù„Ø¬Ù…Ø¹Ø©", "Saturday":"Ø§Ù„Ø³Ø¨Øª"}
        pdf["Ø§Ù„ÙŠÙˆÙ…"] = pdf["Ø§Ù„ÙŠÙˆÙ…"].map(day_map)

        fig_day, ax_day = plt.subplots()
        pdf["Ø§Ù„ÙŠÙˆÙ…"].value_counts().reindex(day_map.values()).plot(kind="bar", ax=ax_day)
        ax_day.set_xticklabels([reshape_label(lbl.get_text()) for lbl in ax_day.get_xticklabels()])
        ax_day.set_xlabel(reshape_label("Ø§Ù„ÙŠÙˆÙ…")); ax_day.set_ylabel(reshape_label("Ø¹Ø¯Ø¯ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª"))
        ax_day.set_title(reshape_label("ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø¹Ù„Ù‰ Ø£ÙŠØ§Ù… Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹"))
        beautify_axes(ax_day); st.pyplot(fig_day)

        pdf["Ø³Ø§Ø¹Ø©"] = pdf["DT"].dt.hour
        fig_hour, ax_hour = plt.subplots()
        pdf["Ø³Ø§Ø¹Ø©"].value_counts().sort_index().plot(kind="bar", ax=ax_hour)
        ax_hour.set_xticklabels([reshape_label(lbl.get_text()) for lbl in ax_hour.get_xticklabels()])
        ax_hour.set_xlabel(reshape_label("Ø§Ù„Ø³Ø§Ø¹Ø©")); ax_hour.set_ylabel(reshape_label("Ø¹Ø¯Ø¯ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª"))
        ax_hour.set_title(reshape_label("ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø¹Ù„Ù‰ Ø³Ø§Ø¹Ø§Øª Ø§Ù„ÙŠÙˆÙ…"))
        beautify_axes(ax_hour); st.pyplot(fig_hour)

        tweets_per_day = pdf.groupby(pdf["DT"].dt.date).size()
        st.metric("Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØºØ±ÙŠØ¯ ÙŠÙˆÙ…ÙŠÙ‹Ø§", f"{tweets_per_day.mean():.2f} ØªØºØ±ÙŠØ¯Ø©/ÙŠÙˆÙ…")
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ±.")

    # =============== ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ ===============
    # =============== ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ (Ù…Ø·ÙˆØ±) ===============
    st.subheader("ğŸ“ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨")
    st.caption("ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ‘Ù„ Ù„Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„ÙƒØªØ§Ø¨Ø©: Ø·ÙˆÙ„ Ø§Ù„Ù†ØµÙˆØµØŒ Ø§Ù„ÙƒÙ„Ù…Ø§ØªØŒ Ø§Ù„Ø¬Ù…Ù„ØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ù…ÙˆØ²ØŒ Ø§Ù„Ù‡Ø§Ø´ØªØ§Ù‚Ø§ØªØŒ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·ØŒ ØªÙ†ÙˆØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§ØªØŒ ÙˆØ§Ù„Ù…Ø´Ø§Ø¹Ø±.")

    style_df = filtered.copy()  # âœ… Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙÙ„ØªØ±Ø©
    if not style_df.empty:
        # --- Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø£Ø³Ø§Ø³ÙŠØ© ---
        style_df["Ø·ÙˆÙ„_Ø§Ù„Ù†Øµ"] = style_df["Ø§Ù„Ù†Øµ"].astype(str).str.len()
        style_df["Ø¹Ø¯Ø¯_Ø§Ù„ÙƒÙ„Ù…Ø§Øª"] = style_df["Ø§Ù„Ù†Øµ"].astype(str).str.split().apply(len)
        style_df["Ø¹Ø¯Ø¯_Ø§Ù„Ø¬Ù…Ù„"] = style_df["Ø§Ù„Ù†Øµ"].str.count(r"[.!ØŸ!]")
        style_df["ÙÙŠÙ‡_Ø³Ø¤Ø§Ù„"] = style_df["Ø§Ù„Ù†Øµ"].str.contains(r"\?", regex=True)
        style_df["ÙÙŠÙ‡_ØªØ¹Ø¬Ø¨"] = style_df["Ø§Ù„Ù†Øµ"].str.contains(r"!", regex=True)
        style_df["ÙÙŠÙ‡_Ù†Ù‚Ø·ØªÙŠÙ†"] = style_df["Ø§Ù„Ù†Øµ"].str.contains(":")
        style_df["Ø¹Ø¯Ø¯_Ø§Ù„Ù‡Ø§Ø´ØªØ§Ù‚Ø§Øª"] = style_df["Ø§Ù„Ù†Øµ"].str.count(r"#\w+")
        style_df["Ø¹Ø¯Ø¯_Ø§Ù„Ø±ÙˆØ§Ø¨Ø·"] = style_df["Ø§Ù„Ù†Øµ"].str.count(r"http[s]?://")
        emoji_pattern = r"[\U0001F300-\U0001FAD6\U0001F900-\U0001F9FF\U00002600-\U000026FF]"
        style_df["Ø¹Ø¯Ø¯_Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ"] = style_df["Ø§Ù„Ù†Øµ"].str.count(emoji_pattern)

        # --- Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ---
        c1, c2, c3 = st.columns(3)
        c1.metric("Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©", f"{style_df['Ø·ÙˆÙ„_Ø§Ù„Ù†Øµ'].mean():.1f} Ø­Ø±Ù")
        c2.metric("Ù…ØªÙˆØ³Ø· Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª", f"{style_df['Ø¹Ø¯Ø¯_Ø§Ù„ÙƒÙ„Ù…Ø§Øª'].mean():.1f}")
        c3.metric("Ù†Ø³Ø¨Ø© Ø§Ù„Ø±Ø¯ÙˆØ¯", f"{style_df['is_reply'].mean()*100:.1f}%")

        c4, c5, c6 = st.columns(3)
        c4.metric("Ù…ØªÙˆØ³Ø· Ø¹Ø¯Ø¯ Ø§Ù„Ø¬Ù…Ù„", f"{style_df['Ø¹Ø¯Ø¯_Ø§Ù„Ø¬Ù…Ù„'].mean():.1f} Ø¬Ù…Ù„Ø©")
        c5.metric("Ù…ØªÙˆØ³Ø· Ø¹Ø¯Ø¯ Ø§Ù„Ù‡Ø§Ø´ØªØ§Ù‚Ø§Øª", f"{style_df['Ø¹Ø¯Ø¯_Ø§Ù„Ù‡Ø§Ø´ØªØ§Ù‚Ø§Øª'].mean():.2f}")
        c6.metric("Ù…ØªÙˆØ³Ø· Ø¹Ø¯Ø¯ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·", f"{style_df['Ø¹Ø¯Ø¯_Ø§Ù„Ø±ÙˆØ§Ø¨Ø·'].mean():.2f}")

        c7, c8, c9 = st.columns(3)
        c7.metric("Ù†Ø³Ø¨Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¹Ø¬Ø¨", f"{style_df['ÙÙŠÙ‡_ØªØ¹Ø¬Ø¨'].mean()*100:.1f}%")
        c8.metric("Ù†Ø³Ø¨Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø¤Ø§Ù„", f"{style_df['ÙÙŠÙ‡_Ø³Ø¤Ø§Ù„'].mean()*100:.1f}%")
        c9.metric("Ù†Ø³Ø¨Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù‚Ø·ØªÙŠÙ†", f"{style_df['ÙÙŠÙ‡_Ù†Ù‚Ø·ØªÙŠÙ†'].mean()*100:.1f}%")

        # --- Ù…Ø¤Ø´Ø± ØªÙ†ÙˆØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ---
        all_words = " ".join(style_df["Ø§Ù„Ù†Øµ"].astype(str)).split()
        unique_words = set(all_words)
        lexical_diversity = len(unique_words) / len(all_words) if len(all_words) > 0 else 0
        st.metric("ğŸ“Š Ù…Ø¤Ø´Ø± ØªÙ†ÙˆØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª", f"{lexical_diversity*100:.1f}%")

        # --- Ø±Ø³Ù… ØªÙˆØ²ÙŠØ¹ Ø·ÙˆÙ„ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª ---
        st.caption("ØªÙˆØ²ÙŠØ¹ Ø·ÙˆÙ„ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª (Ø­Ø±ÙˆÙ).")
        fig_len, ax_len = plt.subplots()
        style_df["Ø·ÙˆÙ„_Ø§Ù„Ù†Øµ"].plot(kind="hist", bins=20, ax=ax_len)
        ax_len.set_xlabel(reshape_label("Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ")); ax_len.set_ylabel(reshape_label("Ø¹Ø¯Ø¯ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª"))
        beautify_axes(ax_len); st.pyplot(fig_len)

        # --- ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹ ---
        import re
        all_emojis = "".join(re.findall(emoji_pattern, " ".join(style_df["Ø§Ù„Ù†Øµ"].astype(str))))
        from collections import Counter
        emoji_counts = Counter(all_emojis)
        if emoji_counts:
            st.markdown("### ğŸ˜ Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹")
            top_emoji = pd.DataFrame(emoji_counts.most_common(10), columns=["Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ","Ø§Ù„ØªÙƒØ±Ø§Ø±"])
            st.dataframe(top_emoji)
        else:
            st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¥ÙŠÙ…ÙˆØ¬ÙŠØ§Øª ÙƒØ§ÙÙŠØ© Ù„Ø¹Ø±Ø¶Ù‡Ø§.")

        # --- Ù…Ù„Ø®Øµ Ù…Ø´Ø§Ø¹Ø± Ø³Ø±ÙŠØ¹ Ø¯Ø§Ø®Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ ---
        st.markdown("### ğŸ˜Š ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± (Ù…Ø¨Ø³Ù‘Ø·)")
        style_df["sentiment"] = style_df["Ø§Ù„Ù†Øµ"].apply(simple_sentiment)
        sent_dist = style_df["sentiment"].map({1:"Ø¥ÙŠØ¬Ø§Ø¨ÙŠ",0:"Ù…Ø­Ø§ÙŠØ¯",-1:"Ø³Ù„Ø¨ÙŠ"}).value_counts(normalize=True)
        st.bar_chart(sent_dist)
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ Ø¨Ø¹Ø¯ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ±.")
    

    # --- Ø¹Ø±Ø¶ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© ÙˆØ§Ù„Ø³Ù„Ø¨ÙŠØ© Ø§Ù„ÙØ¹Ù„ÙŠØ© Ù…Ù† Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª ---
    from collections import Counter

    pos_counter = Counter()
    neg_counter = Counter()

    # --- Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø± ÙÙ‚Ø· ---
    for txt in style_df["Ø§Ù„Ù†Øµ"]:
        normalized = normalize_ar(txt)
        words = re.findall(r"[a-zA-Z\u0600-\u06FF]+", normalized)
        for w in words:
            if w in AR_POS:
                pos_counter[w] += 1
            elif w in AR_NEG:
                neg_counter[w] += 1

    # --- Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· ---
    st.subheader("ğŸ“Š Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© ÙˆØ§Ù„Ø³Ù„Ø¨ÙŠØ© ÙÙŠ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø§Ù„Ù…ÙÙ„ØªØ±Ø©")
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("### âœ… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹")
        if pos_counter:
            pos_df = pd.DataFrame(pos_counter.most_common(20), columns=["Ø§Ù„ÙƒÙ„Ù…Ø©","Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±"])
            st.dataframe(pos_df)
        else:
            st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ ÙƒÙ„Ù…Ø§Øª Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© ÙÙŠ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø§Ù„Ù…ÙÙ„ØªØ±Ø©.")

    with col2:
        st.markdown("### âŒ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø³Ù„Ø¨ÙŠØ© Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹")
        if neg_counter:
            neg_df = pd.DataFrame(neg_counter.most_common(20), columns=["Ø§Ù„ÙƒÙ„Ù…Ø©","Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±"])
            st.dataframe(neg_df)
        else:
            st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ ÙƒÙ„Ù…Ø§Øª Ø³Ù„Ø¨ÙŠØ© ÙÙŠ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø§Ù„Ù…ÙÙ„ØªØ±Ø©.")




    # =============== ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· ===============
    st.subheader("ğŸ¥ ØªØ£Ø«ÙŠØ± Ø§Ù„ÙˆØ³Ø§Ø¦Ø·")

    media_grp = filtered.groupby("has_media")["Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"].mean().rename({False: reshape_label("Ø¨Ø¯ÙˆÙ† ÙˆØ³Ø§Ø¦Ø·"), True: reshape_label("Ù…Ø¹ ÙˆØ³Ø§Ø¦Ø·")})

    if not media_grp.empty:
        fig_media, ax_media = plt.subplots()
        media_grp.plot(kind="bar", ax=ax_media)
        ax_media.set_xlabel(reshape_label("Ø§Ù„ÙˆØ³Ø§Ø¦Ø·"))
        ax_media.set_ylabel(reshape_label("Ù…ØªÙˆØ³Ø· Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"))
        beautify_axes(ax_media)
        st.pyplot(fig_media)
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø¨Ø¹Ø¯ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ±.")


    # =============== ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‡Ø§Ø´ØªØ§Ù‚Ø§Øª ===============
    st.subheader("ğŸ·ï¸ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù‡Ø§Ø´ØªØ§Ù‚Ø§Øª")
    tag_rows = []
    for _, r in filtered.iterrows():  # âœ… Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ÙÙ„ØªØ±
        tags = re.findall(r"#(\w+)", str(r["Ø§Ù„Ù†Øµ"]))
        for t in tags:
            tag_rows.append({"hashtag": t.lower(), "eng": r["Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"]})
    if tag_rows:
        tag_df = pd.DataFrame(tag_rows)
        top_use = tag_df["hashtag"].value_counts().head(15)
        st.bar_chart(top_use)

        perf = tag_df.groupby("hashtag")["eng"].mean().sort_values(ascending=False).head(15)
        st.markdown("### ğŸ” Ø£ÙØ¶Ù„ 15 Ù‡Ø§Ø´ØªØ§Ù‚ Ø­Ø³Ø¨ **Ù…ØªÙˆØ³Ø· Ø§Ù„ØªÙØ§Ø¹Ù„**")
        st.dataframe(perf.reset_index().rename(columns={"hashtag":"Ø§Ù„Ù‡Ø§Ø´ØªØ§Ù‚","eng":"Ù…ØªÙˆØ³Ø· Ø§Ù„ØªÙØ§Ø¹Ù„"}))
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù‡Ø§Ø´ØªØ§Ù‚Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„.")



    # =============== ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ===============
    st.subheader("ğŸ”— ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·")
    st.caption("Ø£ÙƒØ«Ø± Ø§Ù„Ù†Ø·Ø§Ù‚Ø§Øª ØªÙƒØ±Ø§Ø±Ù‹Ø§ ÙÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ ØªØºØ±ÙŠØ¯Ø§ØªÙƒ.")
    all_links = [l for links in pdf["links"] for l in links]
    if all_links:
        domains = pd.Series([re.sub(r"https?://(www\.)?", "", l).split("/")[0] for l in all_links])
        dom_counts = domains.value_counts().head(15)
        st.bar_chart(dom_counts)
        st.markdown("### Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù†Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ù‹Ø§")
        st.dataframe(dom_counts.reset_index().rename(columns={"index":"Ø§Ù„Ù†Ø·Ø§Ù‚", 0:"Ø¹Ø¯Ø¯ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·"}))
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø±ÙˆØ§Ø¨Ø· ÙƒØ§ÙÙŠØ©.")

    # =============== Bigram / Trigram ===============
    st.subheader("ğŸ“š Ø£ÙƒØ«Ø± Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª ØªÙƒØ±Ø§Ø±Ø§Ù‹ (Bigram / Trigram)")
    st.caption("Ù†Ø·Ø¨Ù‘Ø¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ ÙˆÙ†Ø²ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆØ§Ù„Ù…Ù†Ø´Ù† ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙˆÙ‚ÙÙŠØ©ØŒ Ø«Ù… Ù†Ø¹Ø±Ø¶ Ø£ÙƒØ«Ø± Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø´ÙŠÙˆØ¹Ø§Ù‹.")

    from collections import Counter

    AR_STOP = {
        "ÙÙŠ","Ø¹Ù„Ù‰","Ù…Ù†","Ø¹Ù†","Ø§Ù„Ù‰","Ø¥Ù„Ù‰","Ùˆ","Ø§Ùˆ","Ø£Ùˆ","Ù…Ø§","Ù„Ø§","Ù‡Ø°Ø§","Ù‡Ø°Ù‡","Ø°Ù„Ùƒ","Ù‡Ø°ÙŠ","Ù‡Ø°ÙŠÙƒ",
        "Ø§Ù†Ø§","Ø£Ù†Ø§","Ø§Ù†Øª","Ø£Ù†Øª","Ù‡Ùˆ","Ù‡ÙŠ","Ù‡Ù…","Ù‡Ù†","Ù…Ø¹","ØªÙ…","Ø¹Ù†","Ù‚Ø¯","ÙƒØ§Ù†","ÙƒØ§Ù†Øª","ÙƒÙŠÙ","Ù„ÙŠØ´","Ù„ÙŠÙ‡",
        "the","a","an","and","or","to","of","for","in","on","with","is","are","am","it","this","that"
    }

    def normalize_ar(text: str) -> str:
        s = str(text)
        s = re.sub(r"http\S+|www\S+|@\S+", " ", s)
        s = re.sub(r"#", " ", s)
        s = re.sub(r"[\u0617-\u061A\u064B-\u0652\u0670\u06D6-\u06ED\u0640]", "", s)
        s = s.replace("Ø£","Ø§").replace("Ø¥","Ø§").replace("Ø¢","Ø§").replace("Ù‰","ÙŠ").replace("Ø¤","Ùˆ").replace("Ø¦","ÙŠ").replace("Ø©","Ù‡")
        s = re.sub(r"\s+", " ", s).strip().lower()
        return s

    def tokenize(text: str):
        s = normalize_ar(text)
        tokens = re.findall(r"[a-zA-Z\u0600-\u06FF]+", s)
        return [t for t in tokens if len(t) >= 2 and t not in AR_STOP]

    def ngrams_from_tokens(tokens, n=2):
        return [" ".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)] if len(tokens) >= n else []

    def count_ngrams(series_text, n=2) -> Counter:
        c = Counter()
        for txt in series_text:
            toks = tokenize(txt)
            c.update(ngrams_from_tokens(toks, n=n))
        return c

    @st.cache_data
    def count_ngrams_cached(series_text, n=2):
        return count_ngrams(series_text, n)

    def plot_top_counter(counter: Counter, title: str, k: int = 10):
        items = counter.most_common(k)
        if not items:
            st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ø¨Ø§Ø±Ø§Øª ÙƒØ§ÙÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ.")
            return
        labels = [reshape_label(t) for t, _ in items]
        values = [v for _, v in items]
        fig, ax = plt.subplots()
        ax.barh(labels, values)
        ax.set_xlabel(reshape_label("Ø§Ù„ØªÙƒØ±Ø§Ø±"))
        ax.set_title(reshape_label(title))
        beautify_axes(ax)
        st.pyplot(fig)
        st.dataframe(pd.DataFrame(items, columns=["Ø§Ù„Ø¹Ø¨Ø§Ø±Ø©","Ø§Ù„ØªÙƒØ±Ø§Ø±"]))

    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯ÙˆØ§Ù„
    bigrams = count_ngrams_cached(pdf["Ø§Ù„Ù†Øµ"], n=2)
    plot_top_counter(bigrams, "Ø£ÙƒØ«Ø± Bigram ØªÙƒØ±Ø§Ø±Ø§Ù‹", k=10)

    trigrams = count_ngrams_cached(pdf["Ø§Ù„Ù†Øµ"], n=3)
    plot_top_counter(trigrams, "Ø£ÙƒØ«Ø± Trigram ØªÙƒØ±Ø§Ø±Ø§Ù‹", k=10)






    # =============== Ø´Ø¨ÙƒØ© Ø§Ù„Ù…Ù†Ø´Ù† (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) ===============
    st.subheader("ğŸ§© Ø´Ø¨ÙƒØ© Ø§Ù„Ù…Ù†Ø´Ù†")
    st.caption("ØªØ¹Ø±Ø¶ Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… Ø°ÙƒØ±Ù‡Ø§ Ø¨ÙƒØ«Ø±Ø©")
    try:
        from pyvis.network import Network
        import streamlit.components.v1 as components
        import matplotlib.colors as mcolors
        import matplotlib.cm as cm

        mention_rows = []
        for _, row in pdf.iterrows():
            for m in row["mentions"]:
                mention_rows.append({"source": USERNAME, "target": m})

        if not mention_rows:
            st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù†Ø´Ù†Ø§Øª ÙƒØ§ÙÙŠØ©.")
        else:
            mdf = pd.DataFrame(mention_rows)
            mention_counts = mdf["target"].value_counts()
            top_n = st.slider("ğŸ” Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶Ø©", 5, 50, 15)
            selected_mentions = mention_counts.head(top_n)
            st.write(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ **{len(selected_mentions)}** Ø­Ø³Ø§Ø¨ Ù„Ù„Ø¹Ø±Ø¶")
            st.bar_chart(selected_mentions)

            net = Network(height="750px", width="100%", bgcolor="#0E1117", font_color="white", directed=True)
            net.add_node(USERNAME, label=f"@{USERNAME}", size=50, color="#FFD700", shape="dot")

            max_count = selected_mentions.max() if len(selected_mentions) > 0 else 1
            norm = plt.Normalize(vmin=selected_mentions.min(), vmax=max_count)
            import matplotlib
            cmap = matplotlib.colormaps.get("coolwarm")

            for account, count in selected_mentions.items():
                net.add_node(
                    account,
                    label=account,
                    size=20 + count * 3,
                    color=mcolors.to_hex(cmap(norm(count))),
                    shape="dot",
                    title=f"Ù…Ø±Ø§Øª Ø§Ù„Ø°ÙƒØ±: {count}"
                )
                net.add_edge(USERNAME, account, color="#AAAAAA", width=2)

            net.save_graph("mentions_network.html")
            st.success("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø³Ù… â€” ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ¹Ø±Ø§Ø¶Ù‡ Ø£Ø¯Ù†Ø§Ù‡ Ø£Ùˆ ØªØ­Ù…ÙŠÙ„Ù‡.")
            with open("mentions_network.html", "r", encoding="utf-8") as f:
                components.html(f.read(), height=800)
            with open("mentions_network.html", "rb") as f:
                st.download_button("ğŸ’¾ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø±Ø³Ù… ÙƒÙ€ HTML", data=f, file_name="mentions_network.html", mime="text/html")
    except Exception:
        st.warning("âš ï¸ Ù„ØªÙØ¹ÙŠÙ„ Ø´Ø¨ÙƒØ© Ø§Ù„Ù…Ù†Ø´Ù†ØŒ Ø«Ø¨Ù‘Øª:  `pip install pyvis`")


